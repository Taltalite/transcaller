import os
import h5py
import pysam
import pod5
import numpy as np
import argparse
import atexit
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed, wait, FIRST_COMPLETED
import time

# --- [é…ç½®å‚æ•°] ---
SIGNAL_LENGTH = 2048 
WINDOW_STRIDE = 1024
HDF5_WRITE_CHUNK_SIZE = 1024 
MAX_LABEL_LEN = 200

# --- [å…³é”®ä¿®æ”¹ 1: ç¢±åŸºæ˜ å°„æ›´æ–°] ---
# N å’Œ n æ˜ å°„ä¸º 0ï¼Œé€šå¸¸ä½œä¸º Padding æˆ– Blank
BASE_TO_INT = {
    'A': 1, 'C': 2, 'G': 3, 'T': 4,
    'a': 1, 'c': 2, 'g': 3, 't': 4,
    'N': 0, 'n': 0
}
PAD_VAL = 0  # å¯¹åº” BASE_TO_INT ä¸­çš„ N/Blank

# --- å…¨å±€å˜é‡ (ç”¨äºå¤šè¿›ç¨‹ç»§æ‰¿) ---
global_fasta_handle = None
global_pod5_lookup = None
global_pod5_reader_cache = {}


def _close_cached_pod5_readers():
    for reader in global_pod5_reader_cache.values():
        try:
            reader.close()
        except Exception:
            pass


atexit.register(_close_cached_pod5_readers)

def worker_init(fasta_path):
    """åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹çš„ FASTA å¥æŸ„"""
    global global_fasta_handle
    global_fasta_handle = pysam.FastaFile(fasta_path)

def process_task(task_data):
    """
    å¤„ç†å•ä¸ª Read çš„æ ¸å¿ƒå‡½æ•°ã€‚
    ä¿®æ­£äº† Stride è§£æå’Œ TS èµ·å§‹ç‚¹å¯¹é½é€»è¾‘ã€‚
    """
    # 1. è§£åŒ…ä»»åŠ¡æ•°æ® (æ³¨æ„ï¼šts_tag å¯¹åº” template_start_offset)
    read_id_str, ref_name, ref_start, ref_end, ts_tag, mv_tag = task_data
    
    worker_stats = {
        "id_not_in_pod5_index": 0, "read_not_found_in_pod5_file": 0,
        "missing_tags": 0, "signal_too_short": 0, "total_windows_processed": 0,
        "window_mad_is_zero": 0, "window_no_bases": 0, "window_label_invalid": 0,
        "dbg_label_is_empty": 0,
        "dbg_label_is_too_long": 0,
        "valid_samples_created": 0,
    }
    samples_list = [] 

    global global_fasta_handle
    global global_pod5_lookup
    global global_pod5_reader_cache
    
    try:
        if read_id_str not in global_pod5_lookup:
            worker_stats["id_not_in_pod5_index"] += 1
            return samples_list, worker_stats

        pod5_path, batch_idx, row_idx = global_pod5_lookup[read_id_str]
        
        # 2. è·å–å‚è€ƒåºåˆ—
        # æ³¨æ„ï¼šBAM ä¸­çš„åºåˆ—å¯èƒ½åŒ…å« soft clippingï¼Œè¿™é‡Œæœ€å¥½ç›´æ¥ç”¨ BAM query sequence 
        # å¦‚æœä½ åšæŒç”¨ FASTA å‚è€ƒåºåˆ—ï¼Œè¯·ç¡®ä¿ ref_start/end ä¸ä¿¡å·æ˜¯å®Œå…¨å¯¹é½çš„ã€‚
        # **ä¿®æ­£å»ºè®®**ï¼šè®­ç»ƒ basecaller é€šå¸¸ä½¿ç”¨ BAM ä¸­çš„ query_sequence (å› ä¸ºå®ƒæ˜¯å®é™…æµ‹åˆ°çš„åºåˆ—)ï¼Œ
        # ä½†æ—¢ç„¶ä½ ä¼ äº† ref åæ ‡ï¼Œè¿™é‡Œä¿ç•™ä½ åŸæœ¬çš„é€»è¾‘è¯»å– FASTAã€‚
        ground_truth_label_str = global_fasta_handle.fetch(ref_name, ref_start, ref_end).upper()

        # ğŸš€ ==========================================================
        # ğŸš€ [å…³é”®ä¿®æ”¹ 2 & 3]ï¼šMove Table è§£æä¸åæ ‡è®¡ç®—
        # ==========================================================
        
        # A. è§£æ MV æ ‡ç­¾
        raw_mv = np.array(mv_tag, dtype=np.int64)
        
        # è‡ªåŠ¨è·å– Stride (æ ¹æ®ä½ çš„å‘ç°ï¼Œmv[0] æ˜¯ 6)
        stride = raw_mv[0] 
        
        # è·å–å®é™…çš„ moves (0/1 åºåˆ—)
        moves = raw_mv[1:]
        
        # B. æ‰¾åˆ°æ‰€æœ‰å‘ç”Ÿç¢±åŸºè½¬æ¢çš„æ—¶é—´æ­¥ (Frame Indices)
        # np.flatnonzero(moves) è¿”å›çš„æ˜¯ moves æ•°ç»„ä¸­å€¼ä¸º 1 çš„ç´¢å¼•ä½ç½®
        # ä¾‹å¦‚: moves=[1, 0, 1] -> indices=[0, 2]
        base_frame_indices = np.flatnonzero(moves)
        
        # C. ç»“åˆ TS æ ‡ç­¾è®¡ç®—ç»å¯¹é‡‡æ ·ç‚¹åæ ‡
        # ts_tag: Read åœ¨åŸå§‹ä¿¡å·ä¸­çš„ç»å¯¹èµ·å§‹ç‚¹
        # å…¬å¼: ç»å¯¹åæ ‡ = TS + (Frame_Index * Stride)
        if ts_tag is None:
            # å¦‚æœæ²¡æœ‰ ts æ ‡ç­¾ï¼Œå›é€€åˆ° 0 (ä½†åœ¨ä½ çš„æ•°æ®ä¸­åº”è¯¥éƒ½æœ‰)
            ts_offset = 0
        else:
            ts_offset = ts_tag

        base_signal_starts_absolute = ts_offset + (base_frame_indices * stride)

        # å®Œæ•´æ€§æ£€æŸ¥ï¼šç¡®ä¿è®¡ç®—å‡ºçš„ç¢±åŸºæ•°é‡ä¸åºåˆ—é•¿åº¦å¤§è‡´åŒ¹é…
        # len(base_signal_starts_absolute) åº”è¯¥ç­‰äº (æˆ–éå¸¸æ¥è¿‘) len(ground_truth_label_str)
        # å¦‚æœä½ æ˜¯ä» FASTA è·å–çš„åºåˆ—ï¼Œå¯èƒ½ä¼šæœ‰ Indel å¯¼è‡´çš„é•¿åº¦å·®å¼‚ï¼Œè¿™é‡Œä¸åšå¼ºè¡Œ Assertï¼Œä½†è¯·ç•™æ„ã€‚

        # ğŸš€ ==========================================================

        # 3. æ‰“å¼€ POD5 è¯»å–ä¿¡å·
        reader = global_pod5_reader_cache.get(pod5_path)
        if reader is None:
            reader = pod5.Reader(pod5_path)
            global_pod5_reader_cache[pod5_path] = reader

        batch = reader.get_batch(batch_idx)
        pod5_read = batch.get_read(row_idx)

        if pod5_read is None:
            worker_stats["read_not_found_in_pod5_file"] += 1
            return samples_list, worker_stats

        raw_signal = pod5_read.signal

        if len(raw_signal) < SIGNAL_LENGTH:
            worker_stats["signal_too_short"] += 1
            return samples_list, worker_stats

        # 4. æ»‘åŠ¨çª—å£å¤„ç†
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šæˆ‘ä»¬åœ¨ raw_signal ä¸Šæ»‘åŠ¨ï¼Œåˆ‡å‡ºä¸€æ®µä¿¡å·
        # ç„¶åæŸ¥çœ‹ base_signal_starts_absolute ä¸­æœ‰å“ªäº›ç‚¹è½åœ¨è¿™ä¸ªçª—å£å†…
            
        total_bases = len(base_signal_starts_absolute)
        left_idx = 0
        right_idx = 0

        for win_start in range(0, len(raw_signal) - SIGNAL_LENGTH, WINDOW_STRIDE):
            worker_stats["total_windows_processed"] += 1
            win_end = win_start + SIGNAL_LENGTH

            signal_window = raw_signal[win_start:win_end]

            # å½’ä¸€åŒ–
            median = np.median(signal_window)
            mad = np.median(np.abs(signal_window - median))

            if mad == 0:
                worker_stats["window_mad_is_zero"] += 1
                continue

            normalized_signal = (signal_window - median) / mad

            # 5. æ ‡ç­¾å¯¹é½ (Label Alignment)
            # å¢é‡ç§»åŠ¨æŒ‡é’ˆï¼Œé¿å…å¯¹ searchsorted çš„é‡å¤è°ƒç”¨
            while left_idx < total_bases and base_signal_starts_absolute[left_idx] <= win_start:
                left_idx += 1
            while right_idx < total_bases and base_signal_starts_absolute[right_idx] < win_end:
                right_idx += 1

            first_base_idx = left_idx
            last_base_idx = right_idx

            if first_base_idx >= last_base_idx:
                worker_stats["window_no_bases"] += 1
                continue

            # åˆ‡ç‰‡è·å–å¯¹åº”çš„ç¢±åŸºåºåˆ—
            # æ³¨æ„ï¼šå¦‚æœ ref_seq é•¿åº¦ä¸ mv æ¨å¯¼å‡ºçš„ bases æ•°é‡ä¸ä¸€è‡´ï¼Œè¿™é‡Œå¯èƒ½ä¼šè¶Šç•Œï¼ŒåŠ ä¸ªä¿æŠ¤
            current_ref_len = len(ground_truth_label_str)
            safe_last = min(last_base_idx, current_ref_len)

            if first_base_idx >= safe_last:
                continue

            label_str_window = ground_truth_label_str[first_base_idx:safe_last]

            # è½¬æ¢å­—ç¬¦åˆ°æ•´æ•°
            label_int_window = [BASE_TO_INT[b] for b in label_str_window if b in BASE_TO_INT]

            if not label_int_window:
                worker_stats["dbg_label_is_empty"] += 1
                worker_stats["window_label_invalid"] += 1
                continue

            if len(label_int_window) > MAX_LABEL_LEN:
                worker_stats["dbg_label_is_too_long"] += 1
                worker_stats["window_label_invalid"] += 1
                continue

            # 6. Padding (ä½¿ç”¨ 0 å¡«å……)
            padded_label = np.full((MAX_LABEL_LEN,), PAD_VAL, dtype=np.int32)
            padded_label[:len(label_int_window)] = label_int_window

            normalized_signal = normalized_signal.reshape(1, SIGNAL_LENGTH)

            samples_list.append((normalized_signal, padded_label, len(label_int_window)))
            worker_stats["valid_samples_created"] += 1

    except KeyError:
        worker_stats["missing_tags"] += 1
    except Exception as e:
        # æ•è·å…¶ä»–æ½œåœ¨é”™è¯¯é˜²æ­¢è¿›ç¨‹å´©æºƒ
        # print(f"Error processing {read_id_str}: {e}") 
        pass
    
    return samples_list, worker_stats


def write_chunk_to_hdf5(datasets, chunk):
    if not chunk:
        return
    event_ds, label_ds, label_len_ds = datasets
    current_size = event_ds.shape[0]
    new_size = current_size + len(chunk)
    
    event_ds.resize(new_size, axis=0)
    label_ds.resize(new_size, axis=0)
    label_len_ds.resize(new_size, axis=0)
    
    # é¢„åˆ†é… numpy æ•°ç»„ä»¥åŠ é€Ÿå†™å…¥
    chunk_len = len(chunk)
    signals = np.zeros((chunk_len, 1, SIGNAL_LENGTH), dtype=np.float32)
    labels = np.zeros((chunk_len, MAX_LABEL_LEN), dtype=np.int32)
    lengths = np.zeros((chunk_len,), dtype=np.int32)

    for i, (sig, lab, length) in enumerate(chunk):
        signals[i] = sig
        labels[i] = lab
        lengths[i] = length

    event_ds[current_size:new_size] = signals
    label_ds[current_size:new_size] = labels
    label_len_ds[current_size:new_size] = lengths


def consume_completed_futures(completed_futures, futures_set, total_stats, results_chunk, hdf5_datasets):
    if not completed_futures:
        return results_chunk

    for future in completed_futures:
        futures_set.remove(future)
        samples_list, worker_stats = future.result()

        for key, value in worker_stats.items():
            total_stats[key] += value

        if samples_list:
            results_chunk.extend(samples_list)

    if len(results_chunk) >= HDF5_WRITE_CHUNK_SIZE:
        write_chunk_to_hdf5(hdf5_datasets, results_chunk)
        return []

    return results_chunk


def main(args):
    global global_pod5_lookup
    
    total_stats = {
        "bam_reads_processed": 0, "id_not_in_pod5_index": 0, "read_not_found_in_pod5_file": 0,
        "missing_tags": 0, "signal_too_short": 0, "total_windows_processed": 0,
        "window_mad_is_zero": 0, "window_no_bases": 0, "window_label_invalid": 0,
        "dbg_label_is_empty": 0,
        "dbg_label_is_too_long": 0,
        "valid_samples_created": 0, "tasks_submitted": 0
    }
    
    start_time = time.time()

    print("Step 1: Building detailed (path, batch, row) index from POD5 files...")
    pod5_files = [os.path.join(args.pod5_dir, f) for f in os.listdir(args.pod5_dir) if f.endswith('.pod5')]
    
    global_pod5_lookup = {} 
    
    for pod5_path in tqdm(pod5_files, desc="Indexing POD5 files"):
        with pod5.Reader(pod5_path) as reader:
            for batch_idx in range(reader.batch_count):
                batch = reader.get_batch(batch_idx)
                for row_idx in range(batch.num_reads): 
                    read_record = batch.get_read(row_idx) 
                    read_id_str = str(read_record.read_id) 
                    global_pod5_lookup[read_id_str] = (pod5_path, batch_idx, row_idx)
                    
    print(f"Indexed {len(global_pod5_lookup)} unique reads from POD5 files.")

    print("Step 2: Setting up HDF5 file and process pool...")
    bam_file = pysam.AlignmentFile(args.bam_file, "rb")
    # æœ‰äº› BAM æ²¡æœ‰ mapped å±æ€§ï¼Œæˆ–è€…éå¸¸å¤§ï¼Œç”¨ try-except æ›´ç¨³å¥
    try:
        bam_file_size = bam_file.mapped if bam_file.mapped > 0 else 100000
    except:
        bam_file_size = 100000 # Dummy value for tqdm
    
    max_workers = args.workers
    MAX_QUEUE_SIZE = max_workers * 10
    
    print(f"Using {max_workers} worker processes.")

    with h5py.File(args.output_hdf5, 'w') as hf:
        event_ds = hf.create_dataset('event', (0, 1, SIGNAL_LENGTH), maxshape=(None, 1, SIGNAL_LENGTH), dtype=np.float32)
        label_ds = hf.create_dataset('label', (0, MAX_LABEL_LEN), maxshape=(None, MAX_LABEL_LEN), dtype=np.int32)
        label_len_ds = hf.create_dataset('label_len', (0,), maxshape=(None,), dtype=np.int32)
        
        hdf5_datasets = (event_ds, label_ds, label_len_ds)
        results_chunk = []

        with ProcessPoolExecutor(
            max_workers=max_workers,
            initializer=worker_init,
            initargs=(args.reference_fasta,)
        ) as executor:

            futures = set()
            print("Step 3 & 4: Submitting tasks and consuming results...")
            
            # è¿­ä»£ BAM æ–‡ä»¶
            for read in tqdm(bam_file, desc="Processing Reads"):
                total_stats["bam_reads_processed"] += 1
                
                try:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ ‡ç­¾
                    if not read.has_tag('mv') or not read.has_tag('ts'):
                        total_stats["missing_tags"] += 1
                        continue

                    task_data = (
                        read.query_name,
                        read.reference_name,
                        read.reference_start,
                        read.reference_end,
                        read.get_tag('ts'), # ä¼ é€’ TS æ ‡ç­¾
                        read.get_tag('mv')  # ä¼ é€’ MV æ ‡ç­¾
                    )
                    
                    # ç§»é™¤äº† stride å‚æ•°ï¼Œå› ä¸ºç°åœ¨ä» mv[0] è‡ªåŠ¨è·å–
                    futures.add(executor.submit(process_task, task_data))
                    total_stats["tasks_submitted"] += 1
                    
                except Exception as e:
                    # print(f"Skipping read due to error: {e}")
                    continue
                
                # æ¶ˆè´¹è€…é€»è¾‘
                while len(futures) >= MAX_QUEUE_SIZE:
                    done_futures, _ = wait(futures, timeout=0, return_when=FIRST_COMPLETED)
                    if not done_futures:
                        done_futures, _ = wait(futures, return_when=FIRST_COMPLETED)

                    results_chunk = consume_completed_futures(
                        done_futures,
                        futures,
                        total_stats,
                        results_chunk,
                        hdf5_datasets,
                    )

                # æŠ¢å…ˆæ¶ˆè´¹å·²ç»å®Œæˆçš„ä»»åŠ¡ï¼Œé¿å…åœ¨ä¸»å¾ªç¯æœ«å°¾å †ç§¯
                if futures:
                    done_futures, _ = wait(futures, timeout=0, return_when=FIRST_COMPLETED)
                    results_chunk = consume_completed_futures(
                        done_futures,
                        futures,
                        total_stats,
                        results_chunk,
                        hdf5_datasets,
                    )

            # Step 5: å¤„ç†å‰©ä½™ä»»åŠ¡
            print("Step 5: Consuming remaining tasks...")
            remaining_futures = list(futures)
            futures.clear()
            for future in tqdm(as_completed(remaining_futures), total=len(remaining_futures)):
                samples_list, worker_stats = future.result()
                for key, value in worker_stats.items():
                    total_stats[key] += value
                
                if samples_list:
                    results_chunk.extend(samples_list)
                
                if len(results_chunk) >= HDF5_WRITE_CHUNK_SIZE:
                    write_chunk_to_hdf5(hdf5_datasets, results_chunk)
                    results_chunk = [] 

            if results_chunk:
                write_chunk_to_hdf5(hdf5_datasets, results_chunk)

    bam_file.close()
    end_time = time.time()
    print("\n--- PROCESSING FINISHED ---")
    print(f"Total time taken: {end_time - start_time:.2f} seconds")
    print(f"Final valid samples created: {total_stats['valid_samples_created']}")
    print("\n--- DETAILED STATISTICS REPORT ---")
    for key, value in total_stats.items():
        print(f"{key:<30}: {value}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--bam_file", type=str, required=True)
    parser.add_argument("--pod5_dir", type=str, required=True)
    parser.add_argument("--reference_fasta", type=str, required=True)
    parser.add_argument("--output_hdf5", type=str, required=True)
    parser.add_argument("--workers", type=int, default=8)
    # ç§»é™¤äº† basecaller-stride å‚æ•°ï¼Œå› ä¸ºä»£ç ç°åœ¨ä¼šè‡ªåŠ¨è¯†åˆ«
    args = parser.parse_args()
    
    main(args)