#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
test_from_bonito_v3.py

è¿™ä¸ªè„šæœ¬åŠ è½½ä¸€ä¸ªè®­ç»ƒå¥½çš„ TranscallerLight checkpointï¼Œ
åœ¨éªŒè¯é›†ä¸Šè¿è¡Œå®ƒï¼Œå¹¶è®¡ç®—è¯¦ç»†çš„æŒ‡æ ‡ (F1, Error Rates)
ä»¥åŠç”Ÿæˆä¸€ä¸ª *æ±‡æ€»æ‰€æœ‰æ ·æœ¬* çš„å¯¹é½å¯†åº¦çƒ­åŠ›å›¾ã€‚
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import os
import argparse
from tqdm import tqdm
import random
import sys
import Levenshtein # ç¡®ä¿å·²å®‰è£…: pip install python-Levenshtein
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm # <-- å¯¼å…¥ LogNorm

# --- å…³é”®å¯¼å…¥ (ä¸è®­ç»ƒè„šæœ¬ç›¸åŒ) ---
try:
    sys.path.append('/home/lijy/workspace/')
    from my_basecaller.model.transcaller_light import TranscallerLight
except ImportError:
    print("="*80)
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ 'TranscallerLight'ã€‚")
    print("="*80)
    exit(1)

# --- è¾…åŠ©å‡½æ•° (ä¸è®­ç»ƒè„šæœ¬ç›¸åŒ) ---

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Global seed set to {seed}")

# --- æ•°æ®é›†ç±» (ä¸è®­ç»ƒè„šæœ¬ç›¸åŒ) ---
class BonitoNpyDataset(Dataset):
    def __init__(self, data_dir, num_samples_to_load=-1):
        super().__init__()
        
        chunks_path = os.path.join(data_dir, "chunks.npy")
        refs_path = os.path.join(data_dir, "references.npy")
        lens_path = os.path.join(data_dir, "reference_lengths.npy")

        print(f"ğŸš€ (æµ‹è¯•) å¼€å§‹å°† Bonito .npy æ•°æ®ä» {data_dir} åŠ è½½åˆ°å†…å­˜...")
        
        try:
            print("  (1/3) æ­£åœ¨åŠ è½½ 'chunks.npy'...")
            events_np = np.load(chunks_path)
            print("  (2/3) æ­£åœ¨åŠ è½½ 'references.npy'...")
            labels_np = np.load(refs_path)
            print("  (3/3) æ­£åœ¨åŠ è½½ 'reference_lengths.npy'...")
            label_lens_np = np.load(lens_path)
            
            if num_samples_to_load > 0:
                print(f"  ...æˆªå–å‰ {num_samples_to_load} ä¸ªæ ·æœ¬ã€‚")
                events_np = events_np[:num_samples_to_load]
                labels_np = labels_np[:num_samples_to_load]
                label_lens_np = label_lens_np[:num_samples_to_load]

            print("  æ­£åœ¨å°†æ•°æ®è½¬æ¢ä¸º Tensors...")
            self.events = torch.from_numpy(events_np).float().unsqueeze(1)
            self.labels = torch.from_numpy(labels_np).long()
            self.label_lens = torch.from_numpy(label_lens_np).long()
            
            print(f"  æ­£åœ¨è½¬æ¢æ ‡ç­¾ç¼–ç  (Bonito 1-4,0 -> 0-3,4)...")
            self.labels = self.labels - 1
            self.labels[self.labels == -1] = 4 # 4 æ˜¯æˆ‘ä»¬çš„ BLANK_ID
            
            self.dataset_len = self.events.shape[0]
            print(f"ğŸš€ (æµ‹è¯•) æ•°æ®å·²å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ã€‚æ€»æ ·æœ¬æ•°: {self.dataset_len}")
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®åˆ°å†…å­˜æ—¶å‡ºé”™: {e}")
            raise

    def __len__(self):
        return self.dataset_len

    def __getitem__(self, idx):
        return self.events[idx], self.labels[idx], self.label_lens[idx]

# --- CTC è´ªå©ªè§£ç å™¨ (ä¸ä¹‹å‰ç›¸åŒ) ---

def ctc_greedy_decode(log_probs, base_map, blank_id=4):
    preds = torch.argmax(log_probs.squeeze(1), dim=-1)
    prev_char_id = -1
    decoded_sequence = []
    for char_id_tensor in preds:
        char_id = char_id_tensor.item()
        if char_id == prev_char_id:
            continue
        if char_id != blank_id:
            decoded_sequence.append(base_map.get(char_id, '?'))
        prev_char_id = char_id
    return "".join(decoded_sequence)

# --- ğŸš€ æ–°å¢: å¯è§†åŒ–å‡½æ•° (çƒ­åŠ›å›¾) ---

def get_normalized_path(opcodes, pred_len, gt_len):
    """
    ä» Levenshtein opcodes ç”Ÿæˆå½’ä¸€åŒ–çš„ (x, y) åæ ‡ã€‚
    """
    if pred_len == 0 or gt_len == 0:
        return np.array([]), np.array([])
        
    path_i = []
    path_j = []
    
    current_i = 0
    current_j = 0

    for tag, i1, i2, j1, j2 in opcodes:
        if tag == 'equal':
            for k in range(i2 - i1):
                path_i.append(current_i + k)
                path_j.append(current_j + k)
        elif tag == 'replace':
            for k in range(i2 - i1):
                path_i.append(current_i + k)
                path_j.append(current_j + k)
        elif tag == 'insert': # é¢„æµ‹æœ‰ï¼ŒçœŸå®æ²¡æœ‰ (y è½´ä¸åŠ¨)
            for k in range(i2 - i1):
                path_i.append(current_i + k)
                path_j.append(current_j)
        elif tag == 'delete': # çœŸå®æœ‰ï¼Œé¢„æµ‹æ²¡æœ‰ (x è½´ä¸åŠ¨)
            for k in range(j2 - j1):
                path_i.append(current_i)
                path_j.append(current_j + k)
        
        current_i = i2
        current_j = j2

    # æ·»åŠ æœ€åä¸€ä¸ªç‚¹
    path_i.append(pred_len)
    path_j.append(gt_len)

    # å½’ä¸€åŒ–
    norm_i = np.array(path_i) / pred_len
    norm_j = np.array(path_j) / gt_len
    
    return norm_i, norm_j

def plot_alignment_heatmap(all_norm_pred, all_norm_gt, output_filename):
    """
    åˆ›å»ºæ‰€æœ‰æ ·æœ¬çš„å¯¹é½å¯†åº¦çƒ­åŠ›å›¾ã€‚
    """
    
    plt.figure(figsize=(10, 8))
    
    # åˆ›å»º 2D ç›´æ–¹å›¾ (çƒ­åŠ›å›¾)
    # bins: æˆ‘ä»¬å°† [0, 1] çš„ç©ºé—´åˆ†æˆ 100x100 çš„æ ¼å­
    # norm=LogNorm(): 
    #   è¿™æ˜¯*æœ€å…³é”®*çš„ä¸€æ­¥ã€‚
    #   å¯¹è§’çº¿ä¸Šçš„å¯†åº¦ä¼šæ¯”é”™è¯¯é«˜å¾—å¤šï¼Œä½¿ç”¨å¯¹æ•°åˆ»åº¦æ‰èƒ½åŒæ—¶çœ‹åˆ°ä¸¤è€…ã€‚
    # plt.hist2d(
    #     all_norm_pred, 
    #     all_norm_gt, 
    #     bins=100, 
    #     cmap='viridis', 
    #     norm=LogNorm(),
    #     range=[[0, 1], [0, 1]] # ç¡®ä¿èŒƒå›´æ˜¯ 0 åˆ° 1
    # )
    
    plt.scatter(
        all_norm_pred, 
        all_norm_gt, 
        s=0.8,           # ç‚¹çš„å¤§å°
        alpha=0.4,         # ç‚¹çš„é€æ˜åº¦ (å…³é”®ï¼)
        c='blue',          # ç‚¹çš„é¢œè‰²
        edgecolors='none', # ç§»é™¤ç‚¹çš„è¾¹ç¼˜
        label='Alignment Path Points'
    )
    
    # ç»˜åˆ¶ä¸€æ¡å®Œç¾çš„ 45Â° çº¢è‰²è™šçº¿ä½œä¸ºå‚è€ƒ
    plt.plot([0, 1], [0, 1], 'r--', alpha=0.4, label='Perfect Alignment (y=x)')
    
    plt.xlabel("Predicted Position (Normalized)")
    plt.ylabel("Ground Truth Position (Normalized)")
    plt.title("Alignment Density Heatmap (All Samples)")
    plt.colorbar(label="Alignment Path Density (Log Scale)")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.gca().set_aspect('equal', adjustable='box') # è®¾ä¸º 1:1 æ¯”ä¾‹
    
    plt.savefig(output_filename)
    print(f"\nå¯è§†åŒ–æ±‡æ€»çƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {output_filename}")


# ==========================================================================================
# æ­¥éª¤ 3: ä¸»æµ‹è¯•å‡½æ•°
# ==========================================================================================

def main(args):
    
    # --- 1. è®¾ç½®ç¯å¢ƒ ---
    set_seed(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    BASE_MAP = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}

    # --- 2. å‡†å¤‡æ•°æ®é›† ---
    print("åŠ è½½æ•°æ®é›†ä¸­...")
    dataset_to_split = BonitoNpyDataset(args.data_dir, args.num_samples)

    val_size = int(len(dataset_to_split) * args.val_split)
    train_size = len(dataset_to_split) - val_size
    
    _, val_dataset = random_split(
        dataset_to_split, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(args.seed)
    )
    
    print(f"  æˆåŠŸéš”ç¦»å‡ºéªŒè¯é›†ã€‚å¤§å°: {len(val_dataset)}")

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers
    )

    # --- 3. åˆå§‹åŒ–å’ŒåŠ è½½æ¨¡å‹ ---
    print("åˆå§‹åŒ–æ¨¡å‹...")
    model = TranscallerLight(
        input_length=args.input_len,
        output_length=args.output_len,
        num_classes=args.num_classes,
        embed_dim=args.embed_dim,
        depth=args.depth,
        num_heads=args.num_heads,
        drop_path_rate=args.drop_path
    ).to(device)

    print(f"æ­£åœ¨ä» {args.checkpoint} åŠ è½½æƒé‡...")
    try:
        checkpoint = torch.load(args.checkpoint, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print("æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½æ¨¡å‹æƒé‡: {e}")
        exit(1)

    model.eval()

    # --- 4. ğŸš€ è¿è¡Œæµ‹è¯• (åŒ…å«æ–°æŒ‡æ ‡) ---
    
    # ç´¯åŠ å™¨
    total_matches = 0
    total_substitutions = 0
    total_insertions = 0
    total_deletions = 0
    total_gt_len = 0
    total_pred_len = 0
    
    # ğŸš€ ç”¨äºæ±‡æ€»å›¾çš„ç´¯åŠ å™¨
    all_norm_pred_coords = []
    all_norm_gt_coords = []
    
    print("\n" + "="*80)
    print(f"å¼€å§‹åœ¨ {len(val_dataset)} ä¸ªéªŒè¯æ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•...")
    print("="*80)

    with torch.no_grad():
        for i, (events, labels, label_lengths) in enumerate(tqdm(val_loader, desc="Testing")):
            
            events = events.to(device, non_blocking=True)
            log_probs = model(events) # (T, B, C)
            
            for j in range(log_probs.shape[1]):
                
                log_probs_T_C = log_probs[:, j, :]
                pred_str = ctc_greedy_decode(log_probs_T_C, BASE_MAP, args.blank_id)
                
                gt_len = label_lengths[j].item()
                gt_tensor = labels[j][:gt_len]
                gt_str = "".join([BASE_MAP.get(b.item(), '?') for b in gt_tensor])
                
                if len(gt_str) == 0: continue 
                
                opcodes = Levenshtein.opcodes(pred_str, gt_str)
                
                total_gt_len += len(gt_str)
                total_pred_len += len(pred_str)

                for tag, i1, i2, j1, j2 in opcodes:
                    if tag == 'equal':
                        total_matches += (i2 - i1)
                    elif tag == 'replace':
                        total_substitutions += (i2 - i1)
                    elif tag == 'insert':
                        total_insertions += (i2 - i1)
                    elif tag == 'delete':
                        total_deletions += (j2 - j1)
                
                # ğŸš€ æ”¶é›†ç”¨äºç»˜å›¾çš„æ•°æ®
                if args.visualize:
                    norm_i, norm_j = get_normalized_path(opcodes, len(pred_str), len(gt_str))
                    all_norm_pred_coords.append(norm_i)
                    all_norm_gt_coords.append(norm_j)

    print("\n" + "="*80)
    print("æµ‹è¯•å®Œæˆã€‚")
    
    # --- 5. ğŸš€ ç»˜åˆ¶æ±‡æ€»å›¾ ---
    if args.visualize and len(all_norm_pred_coords) > 0:
        print("æ­£åœ¨ç”Ÿæˆæ±‡æ€»çƒ­åŠ›å›¾...")
        all_norm_pred = np.concatenate(all_norm_pred_coords)
        all_norm_gt = np.concatenate(all_norm_gt_coords)
        plot_alignment_heatmap(all_norm_pred, all_norm_gt, args.output_name)
    
    # --- 6. ğŸš€ è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆæŒ‡æ ‡ ---
    if total_gt_len > 0 and total_pred_len > 0:
        
        total_errors = total_substitutions + total_insertions + total_deletions
        base_accuracy = (1.0 - (total_errors / total_gt_len)) * 100.0
        
        precision = total_matches / total_pred_len
        recall = total_matches / total_gt_len
        f1 = 2 * (precision * recall) / (precision + recall)
        
        sub_rate = total_substitutions / total_gt_len
        ins_rate = total_insertions / total_gt_len
        del_rate = total_deletions / total_gt_len
        
        print("\n--- ç»¼åˆè¯„ä¼°æŒ‡æ ‡ ---")
        print(f"  ç¢±åŸºå‡†ç¡®ç‡ (Base Accuracy):   {base_accuracy:.2f}%")
        print(f"  F1-Score:                   {f1 * 100.0:.2f}%")
        print(f"  Precision (ç²¾ç¡®ç‡):         {precision * 100.0:.2f}%")
        print(f"  Recall (å¬å›ç‡):            {recall * 100.0:.2f}%")
        
        print("\n--- é”™è¯¯ç‡ç»†åˆ† (å çœŸå®ç¢±åŸº) ---")
        print(f"  æ›¿æ¢é”™è¯¯ (Substitutions):   {sub_rate * 100.0:.2f}%")
        print(f"  æ’å…¥é”™è¯¯ (Insertions):      {ins_rate * 100.0:.2f}%")
        print(f"  åˆ é™¤é”™è¯¯ (Deletions):       {del_rate * 100.0:.2f}%")
        print("\n" + "="*80)
        
    else:
        print("é”™è¯¯ï¼šæ²¡æœ‰å¤„ç†ä»»ä½•æ•°æ®ã€‚")

# ==========================================================================================
# æ­¥éª¤ 7: Argparse å‘½ä»¤è¡Œå‚æ•°
# ==========================================================================================

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è¯„ä¼° Melchior Basecaller (V3 - æ±‡æ€»çƒ­åŠ›å›¾)")
    
    parser.add_argument('--data-dir', type=str, required=True,
                        help="åŒ…å« chunks.npy, references.npy, reference_lengths.npy çš„ç›®å½•")
    parser.add_argument('--checkpoint', type=str, required=True,
                        help="è¦æµ‹è¯•çš„ .pth æ¨¡å‹ checkpoint æ–‡ä»¶ (ä¾‹å¦‚, ./checkpoints_medium_1M/model_best.pth)")
    parser.add_argument('--output-name', type=str, required=True,
                        help="è¾“å‡ºçš„æ–‡ä»¶å")
    
    # --- æ•°æ®é›†å‚æ•° (å¿…é¡»ä¸è®­ç»ƒæ—¶ç›¸åŒ!) ---
    parser.add_argument('--num-samples', type=int, default=-1)
    parser.add_argument('--val-split', type=float, default=0.05)
    parser.add_argument('--seed', type=int, default=42)
    
    # --- æµ‹è¯•å‚æ•° ---
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('--visualize', action='store_true',
                        help="ç”Ÿæˆ *æ±‡æ€»* çš„å¯¹é½çƒ­åŠ›å›¾")
    
    # --- æ¨¡å‹æ¶æ„å‚æ•° (å¿…é¡»ä¸è®­ç»ƒæ—¶ç›¸åŒ!) ---
    parser.add_argument('--input-len', type=int, default=1998)
    parser.add_argument('--output-len', type=int, default=500)
    parser.add_argument('--num-classes', type=int, default=5)
    parser.add_argument('--blank-id', type=int, default=4)
    
    # é»˜è®¤ä½¿ç”¨ "ä¸­ç­‰æ¨¡å‹" å‚æ•°
    parser.add_argument('--embed-dim', type=int, default=512)
    parser.add_argument('--depth', type=int, default=8)
    parser.add_argument('--num-heads', type=int, default=8)
    parser.add_argument('--drop-path', type=float, default=0.1)
    
    args = parser.parse_args()
    
    main(args)