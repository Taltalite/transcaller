#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
train_hdf5.py

è¿™ä¸ªè„šæœ¬ç”¨äºè®­ç»ƒ TranscallerLight æ¨¡å‹ã€‚
æ•°æ®æºï¼šè‡ªå®šä¹‰ç”Ÿæˆçš„ HDF5 æ•°æ®é›†ã€‚
æ˜ å°„å…³ç³»ï¼šA=1, C=2, G=3, T=4, Blank=0ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import os
import time
import argparse
from tqdm import tqdm
import random
import sys
import h5py

# --- å…³é”®å¯¼å…¥ ---
try:
    # è¯·æ ¹æ®æ‚¨çš„å®é™…è·¯å¾„ä¿®æ”¹è¿™é‡Œ
    sys.path.append('/home/lijy/workspace/')
    from my_basecaller.model.transcaller_light import TranscallerLight
except ImportError:
    print("="*80)
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ 'TranscallerLight'ã€‚")
    print("è¯·ç¡®ä¿ /home/lijy/workspace/ è·¯å¾„æ­£ç¡®ï¼Œ")
    print("å¹¶ä¸” 'my_basecaller/model/transcaller_light.py' æ–‡ä»¶å­˜åœ¨ã€‚")
    print("="*80)
    exit(1)

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Global seed set to {seed}")

# ==========================================================================================
# æ­¥éª¤ 1: ğŸš€ æ–°çš„æ•°æ®é›†ç±» (é€‚é… HDF5 & A=1, Blank=0)
# ==========================================================================================

class HDF5Dataset(Dataset):
    """
    é’ˆå¯¹ HDF5 æ–‡ä»¶çš„ PyTorch Datasetã€‚
    é‡‡ç”¨ Lazy Loading æ¨¡å¼ï¼šåªæœ‰åœ¨ __getitem__ è¢«è°ƒç”¨æ—¶æ‰ä»ç£ç›˜è¯»å–æ•°æ®ï¼ŒèŠ‚çœå†…å­˜ã€‚
    """
    def __init__(self, h5_path, num_samples_to_load=-1):
        super().__init__()
        self.h5_path = h5_path
        self.h5_file = None # å¥æŸ„åˆå§‹åŒ–ä¸º None (Worker è¿›ç¨‹ä¸­æ‰“å¼€)
        
        # ä»…ä¸»è¿›ç¨‹æ‰“å¼€ä¸€æ¬¡ä»¥è·å–é•¿åº¦
        if os.path.exists(h5_path):
            with h5py.File(h5_path, 'r') as f:
                self.total_len = f['event'].shape[0]
                print(f"ğŸš€ HDF5 æ•°æ®é›†æ€»æ ·æœ¬æ•°: {self.total_len}")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {h5_path}")
            
        if num_samples_to_load > 0:
            self.use_len = min(num_samples_to_load, self.total_len)
            print(f"   -> é™åˆ¶ä»…ä½¿ç”¨å‰ {self.use_len} ä¸ªæ ·æœ¬ã€‚")
        else:
            self.use_len = self.total_len

    def __len__(self):
        return self.use_len

    def __getitem__(self, idx):
        """
        å…³é”®ï¼šåœ¨ Worker è¿›ç¨‹ä¸­æ‰“å¼€æ–‡ä»¶å¥æŸ„ã€‚
        """
        if self.h5_file is None:
            # swmr=True å…è®¸åœ¨å†™å…¥æ—¶è¯»å–ï¼Œlibver='latest' æé«˜æ€§èƒ½
            self.h5_file = h5py.File(self.h5_path, 'r', swmr=True, libver='latest')
            
        # 1. è¯»å–æ•°æ® (è¿”å›çš„æ˜¯ numpy array)
        # HDF5 shape: (1, 2048) float32
        event = self.h5_file['event'][idx] 
        # HDF5 shape: (MAX_LABEL_LEN,) int32
        label = self.h5_file['label'][idx] 
        # HDF5 shape: scalar int32
        label_len = self.h5_file['label_len'][idx]

        # 2. è½¬æ¢ä¸º Tensor
        event_tensor = torch.from_numpy(event).float() # Shape: (1, 2048)
        label_tensor = torch.from_numpy(label).long()  # Shape: (MAX_LEN,)
        label_len_tensor = torch.tensor(label_len).long()

        # ğŸš€ 3. æ ‡ç­¾æ˜ å°„æ£€æŸ¥
        # ç”¨æˆ·è¦æ±‚: A=1, C=2, G=3, T=4, Blank=0
        # HDF5å­˜å‚¨: A=1, C=2, G=3, T=4, N/Padding=0
        # 
        # ç»“è®º: ä¸éœ€è¦åšä»»ä½•æ•°å­¦è¿ç®—ï¼
        # HDF5 ä¸­çš„ 0 (Padding) åœ¨ CTCLoss ä¸­è‡ªç„¶ä¼šè¢«å¿½ç•¥ (ç”± label_len æ§åˆ¶)ï¼Œ
        # ä¸”æˆ‘ä»¬å°† blank_id è®¾ä¸º 0ï¼Œé€»è¾‘å®Œå…¨è‡ªæ´½ã€‚
        
        return event_tensor, label_tensor, label_len_tensor

    def __del__(self):
        # ææ„æ—¶å…³é—­æ–‡ä»¶å¥æŸ„
        if self.h5_file is not None:
            try:
                self.h5_file.close()
            except:
                pass

# ==========================================================================================
# æ­¥éª¤ 2: è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# ==========================================================================================

static_printed = False

def train_one_epoch(model, criterion, optimizer, data_loader, device, output_len, scheduler_warmup, warmup_steps, global_step):
    model.train()
    total_loss = 0.0
    progress_bar = tqdm(data_loader, desc='[è®­ç»ƒ]', leave=False)
    
    global static_printed
    
    for events, labels, label_lengths in progress_bar:
        events = events.to(device, non_blocking=True)      # (B, 1, 2048)
        labels = labels.to(device, non_blocking=True)      # (B, MAX_LEN)
        label_lengths = label_lengths.to(device, non_blocking=True) # (B,)
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        log_probs = model(events) # Output shape: (T, B, NumClasses)
        
        batch_size = events.shape[0]
        input_lengths = torch.full(size=(batch_size,), fill_value=output_len, dtype=torch.long, device=device)
        
        # --- è°ƒè¯•æ‰“å° (ä»…ä¸€æ¬¡) ---
        if not static_printed:
            print("\n" + "="*80)
            print("--- è°ƒè¯•ï¼šå³å°†é€å…¥ CTCLoss çš„å¼ é‡ (ä»…æ‰“å°ä¸€æ¬¡) ---")
            print(f"  log_probs.shape:   {log_probs.shape} (T, B, C)")
            print(f"  labels.shape:      {labels.shape}")
            print(f"  label_lengths:     Min={label_lengths.min().item()}, Max={label_lengths.max().item()}")
            
            print("\n  --- æ˜ å°„æ£€æŸ¥ (æœŸæœ›: Blank=0, A=1, C=2, G=3, T=4) ---")
            print(f"  labels Min Val:    {labels.min().item()}")
            print(f"  labels Max Val:    {labels.max().item()}")
            
            if labels.min().item() < 0:
                print("  ğŸ”¥ é”™è¯¯ï¼šLabels åŒ…å«è´Ÿæ•°ï¼")
            if labels.max().item() > 4:
                print("  ğŸ”¥ é”™è¯¯ï¼šLabels åŒ…å«å¤§äº 4 çš„æ•°ï¼")
            
            print("="*80 + "\n")
            static_printed = True
        
        # è®¡ç®—æŸå¤±
        loss = criterion(log_probs, labels, input_lengths, label_lengths)
                
        if torch.isinf(loss):
            print("è­¦å‘Š: é‡åˆ° inf æŸå¤±ï¼Œè·³è¿‡æ­¤ batchã€‚")
            continue
            
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # é¢„çƒ­è°ƒåº¦å™¨
        if global_step < warmup_steps:
            scheduler_warmup.step()
        global_step += 1
        
        total_loss += loss.item()
        
        current_lr = optimizer.param_groups[0]['lr']
        progress_bar.set_postfix(loss=f'{loss.item():.4f}', lr=f'{current_lr:.1e}')
            
    avg_loss = total_loss / len(data_loader)
    return avg_loss, global_step

def validate(model, criterion, data_loader, device, output_len):
    model.eval() 
    total_loss = 0.0
    progress_bar = tqdm(data_loader, desc='[éªŒè¯]', leave=False)
    
    with torch.no_grad():
        for events, labels, label_lengths in progress_bar:
            events = events.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            label_lengths = label_lengths.to(device, non_blocking=True)
            
            log_probs = model(events)
            
            batch_size = events.shape[0]
            input_lengths = torch.full(size=(batch_size,), fill_value=output_len, dtype=torch.long, device=device)
            
            loss = criterion(log_probs, labels, input_lengths, label_lengths)
            
            if not torch.isinf(loss):
                total_loss += loss.item()
                progress_bar.set_postfix(loss=f'{loss.item():.4f}')
            
    avg_loss = total_loss / len(data_loader)
    return avg_loss

# ==========================================================================================
# æ­¥éª¤ 3: ä¸»å‡½æ•°
# ==========================================================================================

def main(args):
    
    # --- 1. è®¾ç½®ç¯å¢ƒ ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    
    # ğŸš€ æ¨¡å‹ä¸‹é‡‡æ ·ç‡ (TranscallerLight ç¡¬ç¼–ç  s=2, s=2 -> 4)
    MODEL_DOWNSAMPLE_RATIO = 4 
    
    # ğŸš€ è‡ªåŠ¨æ ¡éªŒè¾“å‡ºé•¿åº¦
    # HDF5 input_len é»˜è®¤ä¸º 2048 -> expected 512
    expected_output_len = args.input_len // MODEL_DOWNSAMPLE_RATIO
    
    if args.output_len != expected_output_len:
        print(f"æç¤º: å°† output-len ä» {args.output_len} è°ƒæ•´ä¸º {expected_output_len} (åŸºäº input_len {args.input_len})")
        args.output_len = expected_output_len
    
    # --- 2. å‡†å¤‡æ•°æ®é›† (ä½¿ç”¨ HDF5) ---
    print(f"åŠ è½½æ•°æ®é›†: {args.hdf5_path}")
    
    # å®ä¾‹åŒ– HDF5Dataset
    full_dataset = HDF5Dataset(args.hdf5_path, args.num_samples)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(len(full_dataset) * args.val_split)
    train_size = len(full_dataset) - val_size
    
    train_dataset, val_dataset = random_split(
        full_dataset, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(args.seed)
    )
    
    print(f"  è®­ç»ƒé›†å¤§å°: {train_size}")
    print(f"  éªŒè¯é›†å¤§å°: {val_size}")

    # åˆ›å»º DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True, 
        num_workers=args.num_workers,
        pin_memory=True,  
        drop_last=True 
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ ---
    print("åˆå§‹åŒ–æ¨¡å‹...")
    print(f"  Num Classes: {args.num_classes} (0=Blank, 1=A, 2=C, 3=G, 4=T)")
    
    model = TranscallerLight(
        input_length=args.input_len,   # 2048
        output_length=args.output_len, # 512
        num_classes=args.num_classes,  # 5
        embed_dim=args.embed_dim,
        depth=args.depth,
        num_heads=args.num_heads,
        drop_path_rate=args.drop_path
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # ğŸš€ å…³é”®ï¼šè®¾ç½® blank=0
    criterion = nn.CTCLoss(blank=args.blank_id, zero_infinity=True)
    
    optimizer = optim.AdamW(model.parameters(), lr=args.lr)
    
    # è°ƒåº¦å™¨
    steps_per_epoch = len(train_loader)
    warmup_steps = steps_per_epoch # é¢„çƒ­ 1 ä¸ª epoch
    
    scheduler_warmup = optim.lr_scheduler.LinearLR(
        optimizer, start_factor=1e-7 / args.lr, end_factor=1.0, total_iters=warmup_steps
    )
    scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', patience=args.scheduler_patience, factor=0.5, verbose=True
    )

    # --- 4. è®­ç»ƒå¾ªç¯ ---
    print(f"å¼€å§‹è®­ç»ƒ... å…± {args.epochs} è½®")
    best_val_loss = float('inf') 
    global_step = 0

    for epoch in range(1, args.epochs + 1):
        print(f"\n--- Epoch {epoch}/{args.epochs} ---")
        start_time = time.time()
        
        # 1. è®­ç»ƒ
        train_loss, global_step = train_one_epoch(
            model, criterion, optimizer, train_loader, device, args.output_len,
            scheduler_warmup, warmup_steps, global_step
        )
        
        # 2. éªŒè¯
        val_loss = validate(model, criterion, val_loader, device, args.output_len)
        
        # 3. æ›´æ–°
        if global_step >= warmup_steps:
             scheduler_plateau.step(val_loss)
        
        elapsed = time.time() - start_time
        print(f"Epoch {epoch} å®Œæˆ. è€—æ—¶: {elapsed:.2f}s")
        print(f"  [æ€»ç»“] è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        # ä¿å­˜ checkpoint
        save_path_latest = os.path.join(args.checkpoint_dir, "model_latest.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
        }, save_path_latest)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_path_best = os.path.join(args.checkpoint_dir, "model_best_hdf5.pth")
            torch.save(model.state_dict(), save_path_best)
            print(f"  (æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {save_path_best})")
            
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆ!")

# ==========================================================================================
# æ­¥éª¤ 4: å‘½ä»¤è¡Œå‚æ•°
# ==========================================================================================

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è®­ç»ƒ TranscallerLight (ä½¿ç”¨è‡ªå®šä¹‰ HDF5 æ•°æ®é›†)")
    
    # --- å…³é”®è·¯å¾„å‚æ•° ---
    parser.add_argument('--hdf5-path', type=str, required=True,
                        help="ä½ çš„ .hdf5 æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--checkpoint-dir', type=str, default="./checkpoints_hdf5",
                        help="ä¿å­˜æ¨¡å‹ checkpoint çš„ç›®å½•")
    
    # --- æ•°æ®é›†æ§åˆ¶ ---
    parser.add_argument('--num-samples', type=int, default=-1,
                        help="è°ƒè¯•ç”¨ï¼šåªä½¿ç”¨å‰ N ä¸ªæ ·æœ¬ã€‚-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ã€‚")
    parser.add_argument('--val-split', type=float, default=0.05,
                        help="éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.05)")
    
    # --- è®­ç»ƒå‚æ•° ---
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch-size', type=int, default=128)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--scheduler-patience', type=int, default=3)

    # --- æ¨¡å‹å‚æ•° (HDF5 ä¸“ç”¨) ---
    parser.add_argument('--input-len', type=int, default=2048,
                        help="è¾“å…¥ä¿¡å·é•¿åº¦ (é»˜è®¤: 2048)")
    parser.add_argument('--output-len', type=int, default=512,
                        help="CTCè¾“å‡ºé•¿åº¦ (é»˜è®¤: 2048/4 = 512)")
    
    # --- ğŸš€ æ˜ å°„é…ç½®: A=1, C=2, G=3, T=4, Blank=0 ---
    parser.add_argument('--num-classes', type=int, default=5,
                        help="ç±»åˆ«æ•° 5 (0,1,2,3,4)")
    parser.add_argument('--blank-id', type=int, default=0,
                        help="Blank æ ‡ç­¾ ID (è®¾ç½®ä¸º 0 ä»¥åŒ¹é…ä½ çš„è¦æ±‚)")
    
    # --- æ¨¡å‹æ¶æ„ ---
    parser.add_argument('--embed-dim', type=int, default=384)
    parser.add_argument('--depth', type=int, default=6)
    parser.add_argument('--num-heads', type=int, default=4)
    parser.add_argument('--drop-path', type=float, default=0.1)
    
    args = parser.parse_args()
    
    if args.seed >= 0:
        set_seed(seed=args.seed)
    
    main(args)