#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
train_dorado_fixed.py

ä¿®æ­£ç‰ˆè®­ç»ƒè„šæœ¬ï¼Œé€‚é… savectc_fixed.py ç”Ÿæˆçš„æ•°æ®é›†æ ¼å¼ã€‚
å…³é”®å˜æ›´ï¼š
1. æ–‡ä»¶ååŒ¹é… (chunks.npy, references.npy)
2. æ¢å¤ unsqueeze(1) ä»¥é€‚é… Conv1d è¾“å…¥ç»´åº¦
3. è°ƒæ•´ CTC Blank ID = 0ï¼Œç›´æ¥å…¼å®¹ A=1,C=2,G=3,T=4,Pad=0 çš„æ•°æ®æ ¼å¼
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import os
import time
import argparse
from tqdm import tqdm
import random
import sys

# --- å¯¼å…¥æ¨¡å‹ ---
try:
    # è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹
    sys.path.append('/home/lijy/workspace/')
    from my_basecaller.model.transcaller_light import TranscallerLight
except ImportError:
    print("="*80)
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ 'TranscallerLight'ã€‚")
    print("è¯·æ£€æŸ¥ sys.path.append çš„è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    print("="*80)
    sys.exit(1)

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Global seed set to {seed}")

# ==========================================================================================
# æ­¥éª¤ 1: æ•°æ®é›†ç±» (ä¿®æ­£æ–‡ä»¶åå’Œå½¢çŠ¶)
# ==========================================================================================

class DoradoNpyDataset(Dataset):
    """
    åŠ è½½ç”± savectc_fixed.py ç”Ÿæˆçš„ .npy æ•°æ®é›†ã€‚
    æ–‡ä»¶å: chunks.npy, references.npy, reference_lengths.npy
    """
    def __init__(self, data_dir, num_samples_to_load=-1):
        super().__init__()
        
        # [ä¿®æ­£ 1] æ–‡ä»¶åå¿…é¡»ä¸ savectc_fixed.py çš„è¾“å‡ºä¸€è‡´
        signals_path = os.path.join(data_dir, "chunks.npy")
        labels_path = os.path.join(data_dir, "references.npy")
        lens_path = os.path.join(data_dir, "reference_lengths.npy")

        print(f"ğŸš€ å¼€å§‹åŠ è½½æ•°æ®é›†: {data_dir}")
        
        try:
            # 1. åŠ è½½ Numpy æ•°ç»„
            print("  (1/3) åŠ è½½ chunks.npy (ä¿¡å·)...")
            events_np = np.load(signals_path) # Shape: (N, 2048), float16
            
            print("  (2/3) åŠ è½½ references.npy (æ ‡ç­¾)...")
            labels_np = np.load(labels_path)  # Shape: (N, max_len), uint8, Pad=0
            
            print("  (3/3) åŠ è½½ reference_lengths.npy (é•¿åº¦)...")
            label_lens_np = np.load(lens_path) # Shape: (N,), uint16
            
            # 2. æˆªå–æ ·æœ¬ (Debug ç”¨)
            if num_samples_to_load > 0:
                print(f"  ...æˆªå–å‰ {num_samples_to_load} ä¸ªæ ·æœ¬ã€‚")
                events_np = events_np[:num_samples_to_load]
                labels_np = labels_np[:num_samples_to_load]
                label_lens_np = label_lens_np[:num_samples_to_load]

            # 3. è½¬æ¢ä¸º Tensor å¹¶è°ƒæ•´å½¢çŠ¶
            print("  æ­£åœ¨è½¬æ¢æ ¼å¼...")
            
            # [ä¿®æ­£ 2] å¢åŠ  Channel ç»´åº¦
            # Numpy shape æ˜¯ (N, 2048)ï¼ŒPyTorch Conv1d éœ€è¦ (N, Channel, Length)
            # æ‰€ä»¥å¿…é¡» unsqueeze(1) å˜æˆ (N, 1, 2048)
            self.events = torch.from_numpy(events_np).float().unsqueeze(1)
            
            # æ ‡ç­¾éƒ¨åˆ†
            # savectc è„šæœ¬ç”Ÿæˆçš„æ•°æ®ï¼šPad=0, A=1, C=2, G=3, T=4
            # æˆ‘ä»¬å°†åœ¨ CTC Loss ä¸­è®¾ç½® blank=0ï¼Œæ‰€ä»¥ä¸éœ€è¦å¯¹æ ‡ç­¾å€¼åšä»»ä½•å‡æ³•æ“ä½œï¼
            self.labels = torch.from_numpy(labels_np).long()
            self.label_lens = torch.from_numpy(label_lens_np).long()
            
            self.dataset_len = self.events.shape[0]
            
            print(f"ğŸš€ æ•°æ®åŠ è½½å®Œæˆã€‚æ ·æœ¬æ•°: {self.dataset_len}")
            print(f"   Input Shape: {self.events.shape} (æœŸæœ›: N, 1, 2048)")
            print(f"   Label Range: Min={self.labels.min()}, Max={self.labels.max()} (æœŸæœ›: 0-4)")
            
        except FileNotFoundError as e:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            print(f"   è¯·æ£€æŸ¥ --data-dir è·¯å¾„ä¸‹æ˜¯å¦æœ‰ chunks.npy ç­‰æ–‡ä»¶")
            raise
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å‡ºé”™: {e}")
            raise

    def __len__(self):
        return self.dataset_len

    # def __getitem__(self, idx):
    #     return self.events[idx], self.labels[idx], self.label_lens[idx]
    def __getitem__(self, idx):
    # 1. è·å–åŸå§‹ä¿¡å· (1, 2048)
        signal = self.events[idx] 
        
        # 2. === æ–°å¢ï¼šé²æ£’å½’ä¸€åŒ– (Robust Normalization) ===
        # ä½¿ç”¨ä¸­ä½æ•°ç»å¯¹åå·® (MAD) æˆ–ç®€å•çš„ (x - mean) / std
        # å¯¹äº Nanopore ä¿¡å·ï¼Œç®€å•çš„ Z-score é€šå¸¸è¶³å¤Ÿï¼š
        mean = signal.mean()
        std = signal.std()
        
        # é˜²æ­¢é™¤ä»¥ 0 (æå°‘æ•°å…¨æ˜¯å¹³ä¿¡å·çš„æƒ…å†µ)
        if std < 1e-5:
            std = 1.0
            
        signal = (signal - mean) / std
        # ===============================================

        return signal, self.labels[idx], self.label_lens[idx]


# ==========================================================================================
# æ­¥éª¤ 2: è®­ç»ƒæ ¸å¿ƒ (å¢åŠ  Loss Debug)
# ==========================================================================================

static_printed = False

def train_one_epoch(model, criterion, optimizer, data_loader, device, output_len, scheduler_warmup, warmup_steps, global_step):
    model.train()
    total_loss = 0.0
    progress_bar = tqdm(data_loader, desc='[è®­ç»ƒ]', leave=False)
    
    for events, labels, label_lengths in progress_bar:
        events = events.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        label_lengths = label_lengths.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        
        # 1. å‰å‘ä¼ æ’­
        # æ¨¡å‹å†…éƒ¨å·²ç»åŒ…å«äº†:
        # a. Head çš„ permute(1, 0, 2) -> å˜ä¸º (Time, Batch, Class)
        # b. F.log_softmax(dim=-1)    -> å˜ä¸º Log Probabilities
        log_probs = model(events) 
        
        # 2. ç»´åº¦æ£€æŸ¥ (å®‰å…¨èµ·è§)
        # ç¡®ä¿ç¬¬ä¸€ç»´æ˜¯ Time (512)ï¼Œç¬¬äºŒç»´æ˜¯ Batch
        if log_probs.shape[1] != events.shape[0]:
            # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œè¯´æ˜æ¨¡å‹ä»£ç æ²¡åŠ è½½å¯¹ï¼Œæˆ–è€… Head æ²¡ permute
            # ä½†æ ¹æ®ä½ æä¾›çš„æ¨¡å‹ä»£ç ï¼Œè¿™é‡Œä¸éœ€è¦ä»»ä½•æ“ä½œ
            pass 

        batch_size = events.shape[0]
        input_lengths = torch.full(size=(batch_size,), fill_value=output_len, dtype=torch.long, device=device)
        
        # 3. è®¡ç®— Loss (ç›´æ¥ä¼ å…¥)
        loss = criterion(log_probs, labels, input_lengths, label_lengths)
                
        if torch.isinf(loss) or torch.isnan(loss):
            print("âš ï¸ Loss is inf/nan")
            continue
            
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        if global_step < warmup_steps:
            scheduler_warmup.step()
        global_step += 1
        
        total_loss += loss.item()
        progress_bar.set_postfix(loss=f'{loss.item():.4f}')
            
    return total_loss / len(data_loader), global_step

def validate(model, criterion, data_loader, device, output_len):
    model.eval() 
    total_loss = 0.0
    progress_bar = tqdm(data_loader, desc='[éªŒè¯]', leave=False)
    
    with torch.no_grad():
        for events, labels, label_lengths in progress_bar:
            events = events.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            label_lengths = label_lengths.to(device, non_blocking=True)
            
            outputs = model(events)
            if outputs.shape[0] == events.shape[0]:
                 log_probs = outputs.permute(2, 0, 1)
            else:
                 log_probs = outputs

            batch_size = events.shape[0]
            input_lengths = torch.full(size=(batch_size,), fill_value=output_len, dtype=torch.long, device=device)
            
            loss = criterion(log_probs, labels, input_lengths, label_lengths)
            
            if not torch.isinf(loss):
                total_loss += loss.item()
                progress_bar.set_postfix(loss=f'{loss.item():.4f}')
            
    avg_loss = total_loss / len(data_loader)
    return avg_loss

# ==========================================================================================
# æ­¥éª¤ 3: ä¸»æµç¨‹ (è®¾ç½® Blank=0)
# ==========================================================================================

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš¡ ä½¿ç”¨è®¾å¤‡: {device}")
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    
    # ç¡®ä¿ Output Length è®¡ç®—æ­£ç¡®
    # å‡è®¾æ¨¡å‹æœ‰ä¸¤å±‚ stride=2 çš„å·ç§¯ï¼Œè¾“å‡ºé•¿åº¦æ˜¯è¾“å…¥/4
    true_output_len = args.input_len // 4
    if args.output_len != true_output_len:
        print(f"âš ï¸  è‡ªåŠ¨ä¿®æ­£ output-len: {args.output_len} -> {true_output_len}")
        args.output_len = true_output_len
    
    # --- æ•°æ®é›† ---
    dataset_to_split = DoradoNpyDataset(args.data_dir, args.num_samples)

    val_size = int(len(dataset_to_split) * args.val_split)
    train_size = len(dataset_to_split) - val_size
    
    # ç¡®ä¿éªŒè¯é›†è‡³å°‘æœ‰ä¸€ä¸ª batchï¼Œé˜²æ­¢æŠ¥é”™
    if val_size < args.batch_size:
        val_size = args.batch_size
        train_size = len(dataset_to_split) - val_size

    train_dataset, val_dataset = random_split(
        dataset_to_split, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(args.seed)
    )
    
    print(f"ğŸ“š è®­ç»ƒé›†: {train_size} | éªŒè¯é›†: {val_size}")

    train_loader = DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True, 
        num_workers=args.num_workers, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=args.batch_size, shuffle=False,
        num_workers=args.num_workers, pin_memory=True
    )

    # --- æ¨¡å‹ ---
    print("ğŸ› ï¸  åˆå§‹åŒ–æ¨¡å‹...")
    model = TranscallerLight(
        input_length=args.input_len,   # 2048
        output_length=args.output_len, # 512
        num_classes=args.num_classes,  # 5
        embed_dim=args.embed_dim,
        depth=args.depth,
        num_heads=args.num_heads,
        drop_path_rate=args.drop_path
    ).to(device)
    
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # [ä¿®æ­£ 3] è®¾ç½® Blank ID = 0
    # æ•°æ®é›†ä¸­: 0=Pad, 1=A, 2=C, 3=G, 4=T
    # CTCLoss è®¾ç½® blank=0 åï¼Œæ¨¡å‹é¢„æµ‹çš„ Index 0 å°†è¢«è§†ä¸º Blankï¼Œ1-4 ä¸ºç¢±åŸºã€‚
    # è¿™ä¸æ•°æ®é›†çš„ç¼–ç å®Œç¾åŒ¹é…ã€‚
    criterion = nn.CTCLoss(blank=0, zero_infinity=True)
    
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    steps_per_epoch = len(train_loader)
    warmup_steps = steps_per_epoch # 1ä¸ª epoch warmup
    
    scheduler_warmup = optim.lr_scheduler.LinearLR(
        optimizer, start_factor=1e-5, end_factor=1.0, total_iters=warmup_steps
    )
    scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', patience=args.scheduler_patience, factor=0.5, verbose=True
    )

    # --- è®­ç»ƒ ---
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ (Total Epochs: {args.epochs})")
    best_val_loss = float('inf') 
    global_step = 0

    for epoch in range(1, args.epochs + 1):
        print(f"\n--- Epoch {epoch}/{args.epochs} ---")
        start_time = time.time()
        
        train_loss, global_step = train_one_epoch(
            model, criterion, optimizer, train_loader, device, args.output_len,
            scheduler_warmup, warmup_steps, global_step
        )
        
        val_loss = validate(model, criterion, val_loader, device, args.output_len)
        
        if global_step >= warmup_steps:
             scheduler_plateau.step(val_loss)
        
        elapsed = time.time() - start_time
        print(f"Epoch {epoch} è€—æ—¶: {elapsed:.1f}s | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        
        # Save checkpoints
        state = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
        }
        torch.save(state, os.path.join(args.checkpoint_dir, "model_latest.pth"))
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), os.path.join(args.checkpoint_dir, "model_best.pth"))
            print(f"   ğŸ† æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Loss: {best_val_loss:.4f})")
            
    print(f"\nâœ… è®­ç»ƒç»“æŸ. æœ€ä½³éªŒè¯ Loss: {best_val_loss:.4f}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train Transcaller (Corrected for SaveCTC Data)")
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--data-dir', type=str, required=True, help="æ•°æ®é›†ç›®å½• (åŒ…å« chunks.npy)")
    parser.add_argument('--checkpoint-dir', type=str, default="./checkpoints_dorado", help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--num-samples', type=int, default=-1, help="è°ƒè¯•ç”¨ï¼šé™åˆ¶åŠ è½½æ ·æœ¬æ•°")
    parser.add_argument('--val-split', type=float, default=0.05, help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument('--num-workers', type=int, default=8, help="Dataloader çº¿ç¨‹æ•°")
    
    # æ¨¡å‹å‚æ•° (é’ˆå¯¹ Dorado é…ç½®)
    parser.add_argument('--input-len', type=int, default=2048, help="chunks.npy çš„æ¯æ¡é•¿åº¦")
    parser.add_argument('--output-len', type=int, default=512, help="æ¨¡å‹è¾“å‡ºæ­¥é•¿ (é€šå¸¸æ˜¯ input/4)")
    parser.add_argument('--num-classes', type=int, default=5, help="ç±»åˆ«æ•° (Blank + ACGT = 5)")
    
    # [ä¿®æ­£ 4] é»˜è®¤ Blank ID æ”¹ä¸º 0
    parser.add_argument('--blank-id', type=int, default=0, help="CTC Blank Index (å¯¹åº” Pad=0)")
    
    # æ¨¡å‹è¶…å‚
    parser.add_argument('--embed-dim', type=int, default=384)
    parser.add_argument('--depth', type=int, default=6)
    parser.add_argument('--num-heads', type=int, default=4)
    parser.add_argument('--drop-path', type=float, default=0.1)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch-size', type=int, default=128)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--scheduler-patience', type=int, default=3)

    args = parser.parse_args()
    
    if args.seed >= 0:
        set_seed(seed=args.seed)
    
    main(args)