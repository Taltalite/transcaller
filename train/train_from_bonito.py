#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
train_from_bonito.py

è¿™ä¸ªè„šæœ¬ç”¨äºè®­ç»ƒ TranscallerLight æ¨¡å‹ï¼Œ
æ•°æ®æºè‡ª `bonito basecaller --save-ctc` ç”Ÿæˆçš„ .npy æ–‡ä»¶ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split, Subset
import numpy as np
import os
import time
import argparse
from tqdm import tqdm
import random
import sys

# --- å…³é”®å¯¼å…¥ ---
try:
    # ç¡®ä¿
    # 1. è·¯å¾„æ­£ç¡®
    # 2. æ‚¨çš„ transcallerlight_model.py æ–‡ä»¶åœ¨è¯¥è·¯å¾„ä¸‹
    sys.path.append('/home/lijy/workspace/')
    from my_basecaller.model.transcaller_light import TranscallerLight
except ImportError:
    print("="*80)
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ 'TranscallerLight'ã€‚")
    print("è¯·ç¡®ä¿ /home/lijy/workspace/ è·¯å¾„æ­£ç¡®ï¼Œ")
    print("å¹¶ä¸” 'my_basecaller/model/transcaller_light.py' æ–‡ä»¶å­˜åœ¨ã€‚")
    print("="*80)
    exit(1)

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Global seed set to {seed}")

# ==========================================================================================
# æ­¥éª¤ 1: ğŸš€ æ–°çš„æ•°æ®é›†ç±» (ç”¨äº Bonito .npy æ–‡ä»¶)
# ==========================================================================================

class BonitoNpyDataset(Dataset):
    """
    (æ–°ç‰ˆæœ¬ - å¿«é€Ÿå†…å­˜åŠ è½½ .npy)
    é€šè¿‡ä¸€æ¬¡å¤§å‹çš„é¡ºåºè¯»å–ï¼Œå°† Bonito --save-ctc çš„ .npy æ–‡ä»¶åŠ è½½åˆ° RAM ä¸­ã€‚
    """
    def __init__(self, data_dir, num_samples_to_load=-1):
        super().__init__()
        
        chunks_path = os.path.join(data_dir, "chunks.npy")
        refs_path = os.path.join(data_dir, "references.npy")
        lens_path = os.path.join(data_dir, "reference_lengths.npy")

        print(f"ğŸš€ å¼€å§‹å°† Bonito .npy æ•°æ®ä» {data_dir} åŠ è½½åˆ°å†…å­˜...")
        
        try:
            # 1. åŠ è½½ Numpy æ•°ç»„
            print("  (1/3) æ­£åœ¨åŠ è½½ 'chunks.npy'...")
            events_np = np.load(chunks_path)
            
            print("  (2/3) æ­£åœ¨åŠ è½½ 'references.npy'...")
            labels_np = np.load(refs_path)
            
            print("  (3/3) æ­£åœ¨åŠ è½½ 'reference_lengths.npy'...")
            label_lens_np = np.load(lens_path)
            
            # 2. å¦‚æœæŒ‡å®šäº† num_samplesï¼Œåˆ™æˆªå–
            if num_samples_to_load > 0:
                print(f"  ...æˆªå–å‰ {num_samples_to_load} ä¸ªæ ·æœ¬ã€‚")
                events_np = events_np[:num_samples_to_load]
                labels_np = labels_np[:num_samples_to_load]
                label_lens_np = label_lens_np[:num_samples_to_load]

            # 3. å°† Numpy æ•°ç»„è½¬æ¢ä¸º Tensors
            print("  æ­£åœ¨å°†æ•°æ®è½¬æ¢ä¸º Tensors...")
            # .npy æ–‡ä»¶çš„ shape æ˜¯ (B, N)ï¼Œæ·»åŠ é€šé“ç»´åº¦ (B, 1, N)
            self.events = torch.from_numpy(events_np).float().unsqueeze(1)
            self.labels = torch.from_numpy(labels_np).long()
            self.label_lens = torch.from_numpy(label_lens_np).long()
            
            # ğŸš€ ==========================================================
            # ğŸš€ å…³é”®ä¿®å¤: è½¬æ¢æ ‡ç­¾ç¼–ç 
            # Bonito ç¼–ç : A=1, C=2, G=3, T=4, Padding=0
            # æˆ‘ä»¬çš„æ¨¡å‹æœŸæœ›: A=0, C=1, G=2, T=3, Blank=4
            # ğŸš€ ==========================================================
            print(f"  æ­£åœ¨è½¬æ¢æ ‡ç­¾ç¼–ç  (Bonito 1-4,0 -> 0-3,4)...")
            
            # æ­¥éª¤ 1: å°† (A=1...T=4) è½¬æ¢ä¸º (A=0...T=3)ã€‚
            #         è¿™ä¼šå°† (Padding=0) å˜ä¸º (Padding=-1)ã€‚
            self.labels = self.labels - 1
            
            # æ­¥éª¤ 2: å°† (Padding=-1) è½¬æ¢ä¸º (Blank=4)ã€‚
            self.labels[self.labels == -1] = 4
            
            print(f"  è½¬æ¢å®Œæˆã€‚")
            # ==========================================================
            
            self.dataset_len = self.events.shape[0]
            
            print(f"ğŸš€ æ•°æ®å·²å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ã€‚æ€»æ ·æœ¬æ•°: {self.dataset_len}")
            print(f"   ä¿¡å·å¼ é‡ shape: {self.events.shape}")
            print(f"   æ ‡ç­¾å¼ é‡ shape: {self.labels.shape}")
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®åˆ°å†…å­˜æ—¶å‡ºé”™: {e}")
            raise

    def __len__(self):
        return self.dataset_len

    def __getitem__(self, idx):
        # ç›´æ¥ä» RAM è¿”å›ï¼Œé€Ÿåº¦æå¿«
        return self.events[idx], self.labels[idx], self.label_lens[idx]


# ==========================================================================================
# æ­¥éª¤ 2: è®­ç»ƒå’ŒéªŒè¯å‡½æ•° (ä¸æ‚¨ä¹‹å‰çš„ä»£ç ç›¸åŒ)
# ==========================================================================================
# (train_one_epoch å’Œ validate å‡½æ•°ä¿æŒä¸å˜)
# (ä¸ºç®€æ´èµ·è§ï¼Œæ­¤å¤„çœç•¥ï¼Œä½†æ‚¨åº”å°†å…¶ç²˜è´´åˆ°æ­¤å¤„)

static_printed = False

def train_one_epoch(model, criterion, optimizer, data_loader, device, output_len, scheduler_warmup, warmup_steps, global_step):
    model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    total_loss = 0.0
    progress_bar = tqdm(data_loader, desc='[è®­ç»ƒ]', leave=False)
    
    global static_printed
    
    for events, labels, label_lengths in progress_bar:
        events = events.to(device, non_blocking=True) # (B, 1, 1998)
        labels = labels.to(device, non_blocking=True) # (B, 288)
        label_lengths = label_lengths.to(device, non_blocking=True) # (B,)
        
        optimizer.zero_grad()
        log_probs = model(events) # (T, B, C) -> (500, B, 5)
        
        batch_size = events.shape[0]
        input_lengths = torch.full(size=(batch_size,), fill_value=output_len, dtype=torch.long, device=device)
        
        if not static_printed:
            print("\n" + "="*80)
            print("--- è°ƒè¯•ï¼šå³å°†é€å…¥ CTCLoss çš„å¼ é‡ (ä»…æ‰“å°ä¸€æ¬¡) ---")
            print(f"  log_probs.shape:   {log_probs.shape}")
            print(f"  labels.shape:      {labels.shape}")
            print(f"  input_lengths.shape: {input_lengths.shape}")
            print(f"  label_lengths.shape: {label_lengths.shape}")
            
            print("\n  --- å…³é”®æ£€æŸ¥ ---")
            print(f"  labels (Min / Max):       {labels.min().item()} / {labels.max().item()}")
            print(f"  label_lengths (Min / Max): {label_lengths.min().item()} / {label_lengths.max().item()}")
            
            print(f"\n  input_lengths (å‰5ä¸ª): {input_lengths[:5]}")
            print(f"  label_lengths (å‰5ä¸ª): {label_lengths[:5]}")
            
            if labels.min().item() < 0:
                print("  ğŸ”¥ è‡´å‘½é”™è¯¯ï¼šåœ¨è®­ç»ƒå¾ªç¯ä¸­å‘ç° 'labels' åŒ…å«è´Ÿå€¼ï¼")
            elif labels.max().item() > 4:
                print("  ğŸ”¥ è‡´å‘½é”™è¯¯ï¼šåœ¨è®­ç»ƒå¾ªç¯ä¸­å‘ç° 'labels' åŒ…å« > 4 çš„å€¼ï¼")
                print("     (0=A, 1=C, 2=G, 3=T, 4=Blank)")
            elif label_lengths.min().item() == 0:
                 print("  ğŸ”¥ è‡´å‘½é”™è¯¯ï¼šåœ¨è®­ç»ƒå¾ªç¯ä¸­å‘ç° 'label_lengths' åŒ…å« 0ï¼")
                 print("     CTCLoss ä¸å…è®¸ 0 é•¿åº¦çš„æ ‡ç­¾ã€‚")
            else:
                 print("  âœ… æ•°æ®çœ‹èµ·æ¥æœ‰æ•ˆã€‚")

            print("="*80 + "\n")
            static_printed = True
        
        loss = criterion(log_probs, labels, input_lengths, label_lengths)
                
        if torch.isinf(loss):
            print("è­¦å‘Š: é‡åˆ° inf æŸå¤±ï¼Œè·³è¿‡æ­¤ batchã€‚")
            continue
            
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        if global_step < warmup_steps:
            scheduler_warmup.step()
        global_step += 1
        
        total_loss += loss.item()
        
        current_lr = optimizer.param_groups[0]['lr']
        progress_bar.set_postfix(loss=f'{loss.item():.4f}', lr=f'{current_lr:.1e}')
            
    avg_loss = total_loss / len(data_loader)
    return avg_loss, global_step

def validate(model, criterion, data_loader, device, output_len):
    model.eval() 
    total_loss = 0.0
    progress_bar = tqdm(data_loader, desc='[éªŒè¯]', leave=False)
    
    with torch.no_grad():
        for events, labels, label_lengths in progress_bar:
            events = events.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            label_lengths = label_lengths.to(device, non_blocking=True)
            
            log_probs = model(events)
            
            batch_size = events.shape[0]
            input_lengths = torch.full(size=(batch_size,), fill_value=output_len, dtype=torch.long, device=device)
            
            loss = criterion(log_probs, labels, input_lengths, label_lengths)
            
            if not torch.isinf(loss):
                total_loss += loss.item()
                progress_bar.set_postfix(loss=f'{loss.item():.4f}')
            
    avg_loss = total_loss / len(data_loader)
    return avg_loss

# ==========================================================================================
# æ­¥éª¤ 3: ä¸»å‡½æ•° (ğŸš€ æ­¤éƒ¨åˆ†å·²ä¿®æ”¹)
# ==========================================================================================

def main(args):
    
    # --- 1. è®¾ç½®ç¯å¢ƒ ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    
    # ğŸš€ æ£€æŸ¥æ¨¡å‹æ•°å­¦
    MODEL_DOWNSAMPLE_RATIO = 4 # ç¡¬ç¼–ç åœ¨ FastEmbedLight ä¸­
    
    # ğŸš€ æ‰‹åŠ¨è®¡ç®—æ­£ç¡®çš„è¾“å‡ºé•¿åº¦ (åŸºäº 1998)
    # L1_out = floor((1998 + 6 - 7)/2) + 1 = 999
    # L2_out = floor((999 + 2 - 3)/2) + 1 = 500
    true_output_len = 512
    
    
    if args.output_len != true_output_len:
        print("="*80)
        print(f"è­¦å‘Š: --output-len å‚æ•° ({args.output_len}) ä¸æ¨¡å‹å®é™…ä¸‹é‡‡æ ·ä¸ç¬¦ã€‚")
        print(f"       è¾“å…¥ 1998ï¼Œæ¨¡å‹ (s=2, s=2) äº§ç”Ÿ {true_output_len} çš„è¾“å‡ºé•¿åº¦ã€‚")
        print(f"       å°†å¼ºåˆ¶ä½¿ç”¨ {true_output_len} è¿›è¡Œ CTC æŸå¤±è®¡ç®—ã€‚")
        print("="*80)
        
        # â€¼ï¸ å¿…é¡»è¦†ç›–ï¼šæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ­£ç¡®çš„é•¿åº¦ (500)
        args.output_len = true_output_len
    
    # --- 2. å‡†å¤‡æ•°æ®é›† (ğŸš€ æ­¤éƒ¨åˆ†é€»è¾‘å·²æ›´æ–°) ---
    print("åŠ è½½æ•°æ®é›†ä¸­...")
    
    # ğŸš€ (Utils 1) - åŠ è½½åˆ° RAM
    print("ğŸš€ å¯åŠ¨ [å¿«é€Ÿå†…å­˜åŠ è½½] æ¨¡å¼...")
    dataset_to_split = BonitoNpyDataset(args.data_dir, args.num_samples)

    actual_input_len = dataset_to_split.events.shape[-1]
    if args.input_len != actual_input_len:
        print("="*80)
        print(f"è­¦å‘Š: æ‚¨çš„ --input-len ({args.input_len}) ä¸æ•°æ®å®é™…é•¿åº¦ ({actual_input_len}) ä¸ä¸€è‡´ã€‚")
        print(f"       å°†è‡ªåŠ¨ä½¿ç”¨å®é™…é•¿åº¦ {actual_input_len}ã€‚")
        print("="*80)
        args.input_len = actual_input_len

    # (Utils 2) - åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(len(dataset_to_split) * args.val_split)
    train_size = len(dataset_to_split) - val_size
    
    train_dataset, val_dataset = random_split(
        dataset_to_split, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(args.seed)
    )
    
    print(f"  è®­ç»ƒé›†å¤§å°: {train_size}")
    print(f"  éªŒè¯é›†å¤§å°: {val_size}")

    # (Utils 3) - åˆ›å»º Dataloader
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True, # å†…å­˜æ•°æ®é›†å¯ä»¥å®‰å…¨åœ° shuffle
        num_workers=args.num_workers,
        pin_memory=True,  
        drop_last=True 
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    
    print("åˆå§‹åŒ–æ¨¡å‹...")
    # ğŸš€ å…³é”®ï¼šä½¿ç”¨ args.input_len (1998) å’Œ args.output_len (500)
    model = TranscallerLight(
        input_length=args.input_len,   # (æ¥è‡ªæ•°æ®, 1998)
        output_length=args.output_len, # (æ¨¡å‹è®¾è®¡, 500)
        num_classes=args.num_classes,  # (A,C,G,T,blank = 5)
        embed_dim=args.embed_dim,
        depth=args.depth,
        num_heads=args.num_heads,
        drop_path_rate=args.drop_path
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    criterion = nn.CTCLoss(blank=args.blank_id, zero_infinity=True)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr)
    
    # è°ƒåº¦å™¨ 1: çº¿æ€§é¢„çƒ­
    steps_per_epoch = len(train_loader)
    warmup_steps = steps_per_epoch # é¢„çƒ­ 1 æ•´ä¸ª epoch
    print(f" æ¯ä¸ª Epoch æ­¥æ•°: {steps_per_epoch}")
    print(f" (å…³é”®) Warmup æ­¥æ•° (1 epoch): {warmup_steps}")
    scheduler_warmup = optim.lr_scheduler.LinearLR(
        optimizer, 
        start_factor=1e-7 / args.lr,
        end_factor=1.0, 
        total_iters=warmup_steps
    )
    
    # è°ƒåº¦å™¨ 2: å¹³å°è¡°å‡
    scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', 
        patience=args.scheduler_patience, 
        factor=0.5, 
        verbose=True
    )

    # --- 4. è®­ç»ƒå¾ªç¯ ---
    print(f"å¼€å§‹è®­ç»ƒ... å…± {args.epochs} è½®")
    best_val_loss = float('inf') 
    global_step = 0

    for epoch in range(1, args.epochs + 1):
        print(f"\n--- Epoch {epoch}/{args.epochs} ---")
        
        start_time = time.time()
        
        # 1. è®­ç»ƒ
        train_loss, global_step = train_one_epoch(
            model, criterion, optimizer, train_loader, device, args.output_len,
            scheduler_warmup, warmup_steps, global_step
        )
        
        # 2. éªŒè¯
        val_loss = validate(model, criterion, val_loader, device, args.output_len)
        
        # 3. æ›´æ–°è°ƒåº¦å™¨
        if global_step >= warmup_steps:
             scheduler_plateau.step(val_loss)
        
        elapsed = time.time() - start_time
        print(f"Epoch {epoch} å®Œæˆ. è€—æ—¶: {elapsed:.2f}s")
        print(f"  [æ€»ç»“] è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        # (Utils 3) - æ¨¡å‹ä¿å­˜
        save_path_latest = os.path.join(args.checkpoint_dir, "model_latest.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_plateau_state_dict': scheduler_plateau.state_dict(),
            'scheduler_warmup_state_dict': scheduler_warmup.state_dict(),
            'val_loss': val_loss,
        }, save_path_latest)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_path_best = os.path.join(args.checkpoint_dir, "model_from_bonito.pth")
            torch.save(model.state_dict(), save_path_best)
            print(f" (æ–°æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.4f}, å·²ä¿å­˜è‡³ {save_path_best})")
            
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {os.path.join(args.checkpoint_dir, 'model_best.pth')}")

# ==========================================================================================
# æ­¥éª¤ 4: Argparse å‘½ä»¤è¡Œå‚æ•° (ğŸš€ å·²æ›´æ–°)
# ==========================================================================================

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è®­ç»ƒ Melchior Basecaller (ä½¿ç”¨ Bonito .npy æ•°æ®é›†)")
    
    # --- æ•°æ®å’Œè·¯å¾„å‚æ•° ---
    parser.add_argument('--data-dir', type=str, required=True,
                        help="åŒ…å« chunks.npy, references.npy, reference_lengths.npy çš„ç›®å½•")
    parser.add_argument('--checkpoint-dir', type=str, default="./checkpoints",
                        help="ä¿å­˜æ¨¡å‹ checkpoint çš„ç›®å½•")
    
    # --- æ•°æ®é›†æ§åˆ¶ ---
    parser.add_argument('--num-samples', type=int, default=-1,
                        help="è¦ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ã€‚ (é»˜è®¤: -1)")
    parser.add_argument('--val-split', type=float, default=0.05, # <-- æ‚¨ä½¿ç”¨çš„æ˜¯ 0.05
                        help="ç”¨äºéªŒè¯é›†çš„æ¯”ä¾‹ (ä¾‹å¦‚ 0.05 è¡¨ç¤º 5%) (é»˜è®¤: 0.05)")
    
    # --- è®­ç»ƒè¶…å‚æ•° ---
    parser.add_argument('--epochs', type=int, default=20,
                        help="è®­ç»ƒè½®æ•° (é»˜è®¤: 20)")
    parser.add_argument('--batch-size', type=int, default=128, # <-- æ‚¨ä½¿ç”¨çš„æ˜¯ 64
                        help="æ‰¹é‡å¤§å° (é»˜è®¤: 128)")
    parser.add_argument('--lr', type=float, default=1e-4, # <-- æ‚¨ä½¿ç”¨çš„æ˜¯ 1e-4
                        help="å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)")
    parser.add_argument('--num-workers', type=int, default=8,
                        help="DataLoader ä½¿ç”¨çš„è¿›ç¨‹æ•° (é»˜è®¤: 8)")
    parser.add_argument('--seed', type=int, default=42,
                        help="éšæœºç§å­ (é»˜è®¤: 42)")
    parser.add_argument('--scheduler-patience', type=int, default=3,
                        help="LR è°ƒåº¦å™¨ç­‰å¾…çš„è½®æ•° (é»˜è®¤: 3)")

    # --- ğŸš€ æ¨¡å‹æ¶æ„å‚æ•° (å¿…é¡»ä¸ Bonito æ•°æ®åŒ¹é…) ---
    parser.add_argument('--input-len', type=int, default=1998,
                        help="è¾“å…¥ä¿¡å·åºåˆ—é•¿åº¦ (!! åŒ¹é… chunks.npy !! é»˜è®¤: 1998)")
    parser.add_argument('--output-len', type=int, default=512,
                        help="æ¨¡å‹è¾“å‡ºåºåˆ—é•¿åº¦ (!! åŒ¹é… 1998/4 a=500 !! é»˜è®¤: 512)")
    parser.add_argument('--num-classes', type=int, default=5,
                        help="ç±»åˆ«æ•° (A,C,G,T,blank) (é»˜è®¤: 5)")
    parser.add_argument('--blank-id', type=int, default=4,
                        help="CTCLoss çš„ç©ºç™½æ ‡ç­¾ ID (é»˜è®¤: 4)")
    
    # --- (å¯é€‰) Transcaller å†…éƒ¨å‚æ•° ---
    parser.add_argument('--embed-dim', type=int, default=384,
                        help="Transformer åµŒå…¥ç»´åº¦ (é»˜è®¤: 384)")
    parser.add_argument('--depth', type=int, default=6,
                        help="Transformer å±‚æ•° (é»˜è®¤: 6)")
    parser.add_argument('--num-heads', type=int, default=4,
                        help="Transformer æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 4)")
    parser.add_argument('--drop-path', type=float, default=0.1,
                        help="éšæœºæ·±åº¦æ¦‚ç‡ (é»˜è®¤: 0.1)")
    
    
    args = parser.parse_args()
    
    if args.seed >= 0:
        set_seed(seed=args.seed)
    
    print("="*80)
    print("è®­ç»ƒé…ç½® (ä½¿ç”¨ Bonito .npy æ•°æ®):")
    for k, v in vars(args).items():
        print(f"  {k}: {v}")
    print("="*80)
    
    main(args)