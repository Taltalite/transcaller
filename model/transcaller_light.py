import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torchinfo import summary
import warnings
from functools import partial
import collections.abc
from itertools import repeat

# ==========================================================================================
# è¾…åŠ©å‡½æ•°å’Œç±» (ä¸æ‚¨æä¾›çš„ä»£ç ç›¸åŒ)
# ==========================================================================================

def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
            return tuple(x)
        return tuple(repeat(x, n))
    return parse

to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)

def _trunc_normal_(tensor, mean, std, a, b):

    def norm_cdf(x):
        return (1. + math.erf(x / math.sqrt(2.))) / 2.
    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)
    tensor.uniform_(2 * l - 1, 2 * u - 1)
    tensor.erfinv_()
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)
    tensor.clamp_(min=a, max=b)
    return tensor

def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob > 0.0 and scale_by_keep:
        random_tensor.div_(keep_prob)
    return x * random_tensor

class DropPath(nn.Module):
    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep
    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)
    def extra_repr(self):
        return f'drop_prob={round(self.drop_prob,3):0.3f}'


class Mlp(nn.Module):
    def __init__(
            self,
            in_features,
            hidden_features=None,
            out_features=None,
            act_layer=nn.GELU,
            norm_layer=None,
            bias=True,
            drop=0.,
            use_conv=False,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        bias = to_2tuple(bias)
        drop_probs = to_2tuple(drop)
        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear
        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])
        self.drop2 = nn.Dropout(drop_probs[1])
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.norm(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        assert dim % num_heads == 0, "dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0) 
        x = F.scaled_dot_product_attention(
            q, k, v, 
            dropout_p=self.attn_drop.p if self.training else 0.0 # æ³¨æ„ï¼šä»…åœ¨è®­ç»ƒæ—¶å¯ç”¨dropout
        )
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., 
                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.mixer = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, 
            attn_drop=attn_drop, proj_drop=drop
        )
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim, 
            hidden_features=mlp_hidden_dim, 
            act_layer=act_layer, 
            drop=drop
        )
    def forward(self, x):
        x = x + self.drop_path(self.mixer(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class Head(nn.Module):
    def __init__(self, in_features, seq_len, out_seq_len, num_classes):
        super().__init__()
        # self.adaptive_pool = nn.AdaptiveAvgPool1d(out_seq_len)
        self.out_proj = nn.Linear(in_features, num_classes)
    def forward(self, x):
        # x = x.transpose(1, 2)
        # x = self.adaptive_pool(x)
        # x = x.transpose(1, 2)
        x = self.out_proj(x)
        return x.permute(1, 0, 2)

# ==========================================================================================
# ğŸš€ æ”¹è¿›åçš„è½»é‡åŒ–æ¨¡å—
# ==========================================================================================

class FastEmbedLight(nn.Module):
    """
    ä¸€ä¸ªåŸºäº 1D å·ç§¯çš„è½»é‡åŒ–åµŒå…¥æ¨¡å—ï¼Œå¢åŠ äº†ä¸‹é‡‡æ ·åŠŸèƒ½ã€‚
    
    å‚æ•°:
        in_chans (int): è¾“å…¥é€šé“æ•° (é€šå¸¸æ˜¯ 1)
        in_dim (int): ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•° (ä¸­é—´ç»´åº¦)
        embed_dim (int): æœ€ç»ˆåµŒå…¥ç»´åº¦ (Transformer çš„éšè—ç»´åº¦ C)
        downsample_ratio (int): ä¸‹é‡‡æ ·ç‡ (ç›®å‰ç¡¬ç¼–ç ä¸º 4x)
    """
    def __init__(self, in_chans=1, in_dim=128, embed_dim=384, downsample_ratio=4):
        super().__init__()
        
        if downsample_ratio != 4:
            # æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å¾ªç¯å’Œå‚æ•°æ¥ä½¿å…¶æ›´çµæ´»
            warnings.warn("è¿™ä¸ª FastEmbedLight ç‰ˆæœ¬ç›®å‰ç¡¬ç¼–ç ä¸º 4x ä¸‹é‡‡æ ·")
            
        self.downsample_ratio = downsample_ratio
        
        # å®šä¹‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ª 1D å·ç§¯å—çš„åºåˆ—
        self.conv_down = nn.Sequential(
            # --- ç¬¬ 1 å— ---
            # (k=7, s=2, p=3) -> åºåˆ—é•¿åº¦å‡åŠ (ä¾‹å¦‚ 2048 -> 1024)
            nn.Conv1d(in_chans, in_dim, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm1d(in_dim, eps=1e-4),
            nn.GELU(approximate='tanh'),

            # --- ç¬¬ 2 å— ---
            # (k=3, s=2, p=1) -> åºåˆ—é•¿åº¦å†æ¬¡å‡åŠ (ä¾‹å¦‚ 1024 -> 512)
            nn.Conv1d(in_dim, embed_dim, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm1d(embed_dim, eps=1e-4),
            nn.GELU(approximate='tanh')
        )

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ã€‚
        è¾“å…¥ x å½¢çŠ¶: (B, C_in, N_in)  -> (B, 1, 2048)
        """
        
        # 1. é€šè¿‡å·ç§¯å—
        # x å½¢çŠ¶: (B, C_in, N_in) -> (B, embed_dim, N_in / 4)
        # ä¾‹å¦‚: (B, 1, 2048) -> (B, 384, 512)
        x = self.conv_down(x)
        
        # 2. è°ƒæ•´ç»´åº¦é¡ºåº
        # x å½¢çŠ¶: (B, embed_dim, N_out) -> (B, N_out, embed_dim)
        # ä¾‹å¦‚: (B, 384, 512) -> (B, 512, 384)
        x = x.transpose(1, 2)
        
        return x


class TranscallerLight(nn.Module):
    """
    è½»é‡åŒ–ç‰ˆæœ¬çš„ Transcaller æ¨¡å‹ã€‚
    
    ä¸»è¦å˜åŒ–:
    1. ä½¿ç”¨ FastEmbedLight è¿›è¡Œ 4x ä¸‹é‡‡æ ·ã€‚
    2. Positional Embedding é•¿åº¦é€‚åº”ä¸‹é‡‡æ ·åçš„åºåˆ—ã€‚
    3. é»˜è®¤å‚æ•° (embed_dim, depth, num_heads, mlp_ratio) å·²è¢«ç¼©å°ã€‚
    """
    def __init__(self, in_chans=1, 
                 embed_dim=384,  # <-- ç¼©å°
                 depth=6,        # <-- ç¼©å°
                 num_heads=4,    # <-- ç¼©å°
                 mlp_ratio=2.0,  # <-- ç¼©å°
                 qkv_bias=False, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 input_length=2048, output_length=420, num_classes=5,
                 downsample_ratio=4): # <-- æ–°å¢å‚æ•°
        
        super().__init__()
        
        # 1. ä½¿ç”¨è½»é‡åŒ– Stem (å¸¦ä¸‹é‡‡æ ·)
        self.stem = FastEmbedLight(
            in_chans=in_chans, 
            in_dim=128, 
            embed_dim=embed_dim, 
            downsample_ratio=downsample_ratio
        )
        
        # 2. è®¡ç®—ä¸‹é‡‡æ ·åçš„åºåˆ—é•¿åº¦
        # self.transformer_seq_len = input_length // downsample_ratio
        
        # ç¬¬ 1 å±‚ Conv1d: (k=7, s=2, p=3) [cite: 17]
        l1_out = math.floor((input_length + 2 * 3 - 7) / 2) + 1

        # ç¬¬ 2 å±‚ Conv1d: (k=3, s=2, p=1) [cite: 18]
        l2_out = math.floor((l1_out + 2 * 1 - 3) / 2) + 1

        self.transformer_seq_len = l2_out # (1998 -> 999 -> 500)
        
        # 3. Positional Embedding é€‚åº”æ–°çš„åºåˆ—é•¿åº¦
        self.pos_embed = nn.Parameter(torch.zeros(1, self.transformer_seq_len, embed_dim))
        trunc_normal_(self.pos_embed, std=.02)

        # 4. éšæœºæ·±åº¦ (Stochastic Depth) è¡°å‡
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        
        # 5. Transformer ç¼–ç å™¨å †æ ˆ (ä½¿ç”¨ç¼©å°çš„ depth, dim, heads, mlp_ratio)
        self.blocks = nn.ModuleList([
            TransformerBlock(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=nn.LayerNorm
            )
            for i in range(depth) # ä½¿ç”¨ç¼©å°çš„ depth
        ])
        
        # 6. æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚
        self.norm = nn.LayerNorm(embed_dim)
        
        # 7. è¾“å‡ºå¤´ (seq_len å‚æ•°ä¼ å…¥ä¸‹é‡‡æ ·åçš„é•¿åº¦)
        self.head = Head(embed_dim, self.transformer_seq_len, output_length, num_classes=num_classes)
        
    def forward(self, x):
        """
        å®Œæ•´çš„å‰å‘ä¼ æ’­ã€‚
        è¾“å…¥ x å½¢çŠ¶: (B, 1, 2048)
        """
        
        # 1. åµŒå…¥ä¸ä¸‹é‡‡æ ·
        # x å½¢çŠ¶: (B, 1, 2048) -> (B, 512, embed_dim)
        x = self.stem(x)
        
        # 2. æ·»åŠ ä½ç½®ç¼–ç 
        # (B, 512, embed_dim) + (1, 512, embed_dim)
        x = x + self.pos_embed
        
        # 3. é€šè¿‡æ‰€æœ‰ Transformer å—
        # x å½¢çŠ¶ä¿æŒä¸å˜: (B, 512, embed_dim)
        for block in self.blocks:
            x = block(x)
            
        # 4. æœ€ç»ˆå½’ä¸€åŒ–
        # x å½¢çŠ¶: (B, 512, embed_dim)
        x = self.norm(x)
        
        # 5. é€šè¿‡è¾“å‡ºå¤´
        # x å½¢çŠ¶: (B, 512, embed_dim) -> (420, B, 5)
        x = self.head(x)
        
        # 6. è®¡ç®—å¯¹æ•°æ¦‚ç‡
        # (CTCLoss æœŸæœ›å¯¹æ•°æ¦‚ç‡ä½œä¸ºè¾“å…¥)
        # è¾“å‡ºå½¢çŠ¶: (420, B, 5)
        return F.log_softmax(x, dim=-1)

# ==========================================================================================
# ä¸»å‡½æ•°ï¼šå®ä¾‹åŒ–å’Œæµ‹è¯• (è½»é‡åŒ–ç‰ˆæœ¬)
# ==========================================================================================
if __name__ == '__main__':
    # --- 1. é…ç½®å’Œå‚æ•° ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # æ¨¡å‹å‚æ•° (ä½¿ç”¨äº†ç¤ºä¾‹ä¸­çš„å‚æ•°)
    BATCH_SIZE = 4 
    
    # ğŸš€ è½»é‡åŒ–è¶…å‚æ•° 
    EMBED_DIM = 384    # åµŒå…¥ç»´åº¦ (åŸ: 768)
    DEPTH = 6          # Transformer å±‚æ•° (åŸ: 12)
    NUM_HEADS = 4      # æ³¨æ„åŠ›å¤´æ•° (åŸ: 6)
    MLP_RATIO = 2.0    # MLP æ¯”ä¾‹ (åŸ: 4.0)
    
    # ä¿¡å·å‚æ•° (ä¿æŒä¸å˜)
    SEQ_LEN = 2048     # åŸå§‹è¾“å…¥åºåˆ—é•¿åº¦
    OUTPUT_LEN = 420   # è¾“å‡ºåºåˆ—é•¿åº¦
    NUM_CLASSES = 5    # åˆ†ç±»æ•° {A, C, G, T, <blank>}

    # --- 2. å®ä¾‹åŒ– (è½»é‡åŒ–) æ¨¡å‹ ---
    model = TranscallerLight(
        input_length=SEQ_LEN,
        embed_dim=EMBED_DIM,
        depth=DEPTH,
        num_heads=NUM_HEADS,
        mlp_ratio=MLP_RATIO,
        output_length=OUTPUT_LEN,
        num_classes=NUM_CLASSES,
        downsample_ratio=4  # æ˜ç¡®æŒ‡å®šä¸‹é‡‡æ ·ç‡
    ).to(device)

    # --- 3. ä½¿ç”¨ torchinfo æ‰“å°æ¶æ„æ‘˜è¦ ---
    print("\n" + "="*80)
    print("è½»é‡åŒ–æ¨¡å‹æ¶æ„æ‘˜è¦ (TranscallerLight)")
    print("="*80)
    
    # å®šä¹‰ torchinfo éœ€è¦çš„è¾“å…¥å¤§å°
    # (batch_size, in_channels, sequence_length)
    input_size = (BATCH_SIZE, 1, SEQ_LEN)
    
    # æ‰“å°æ‘˜è¦
    summary(model, input_size=input_size, device=device,
            col_names=["input_size", "output_size", "num_params", "mult_adds"])
    
    # --- 4. (å¯é€‰) æµ‹è¯•ä¸€æ¬¡å‰å‘ä¼ æ’­ ---
    print("\n" + "="*80)
    print("æµ‹è¯•ä¸€æ¬¡å‰å‘ä¼ æ’­...")
    print(f"åˆ›å»ºéšæœºè¾“å…¥: {input_size}")
    dummy_input = torch.randn(input_size).to(device)
    
    with torch.no_grad(): # å…³é—­æ¢¯åº¦è®¡ç®—
        output = model(dummy_input)
        
    print(f"æ¨¡å‹æˆåŠŸæ‰§è¡Œï¼")
    print(f"æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {output.shape} (åº”ä¸º: {OUTPUT_LEN, BATCH_SIZE, NUM_CLASSES})")
    print("="*80)